{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d04a549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-59.6.0-py3-none-any.whl (952 kB)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Installing collected packages: pip, setuptools, wheel\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.1.1\n",
      "    Uninstalling pip-20.1.1:\n",
      "      Successfully uninstalled pip-20.1.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 49.2.0.post20200714\n",
      "    Uninstalling setuptools-49.2.0.post20200714:\n",
      "      Successfully uninstalled setuptools-49.2.0.post20200714\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Uninstalling wheel-0.34.2:\n",
      "      Successfully uninstalled wheel-0.34.2\n",
      "Successfully installed pip-21.3.1 setuptools-59.6.0 wheel-0.37.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ffcbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv-python-4.7.0.72.tar.gz (91.1 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "Note: you may need to restart the kernel to use updated packages.  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'D:\\anacnda3\\envs\\cudaenvOld\\python.exe' 'D:\\anacnda3\\envs\\cudaenvOld\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\fai-w\\AppData\\Local\\Temp\\tmpzfpfz23u'\n",
      "       cwd: C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\n",
      "  Complete output (584 lines):\n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja (Visual Studio 17 2022 x64 v143)' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "    Generator\n",
      "  \n",
      "      Ninja\n",
      "  \n",
      "    does not support platform specification, but platform\n",
      "  \n",
      "      x64\n",
      "  \n",
      "    was specified.\n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from opencv-python) (1.17.0)\n",
      "Building wheels for collected packages: opencv-python\n",
      "  Building wheel for opencv-python (pyproject.toml): started\n",
      "  Building wheel for opencv-python (pyproject.toml): still running...\n",
      "  Building wheel for opencv-python (pyproject.toml): still running...\n",
      "  Building wheel for opencv-python (pyproject.toml): still running...\n",
      "  Building wheel for opencv-python (pyproject.toml): still running...\n",
      "  Building wheel for opencv-python (pyproject.toml): finished with status 'error'\n",
      "Failed to build opencv-python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja (Visual Studio 17 2022 x64 v143)' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Visual Studio 17 2022 x64 v143' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  -- Selecting Windows SDK version 10.0.19041.0 to target Windows 10.0.19045.\n",
      "  -- The C compiler identification is MSVC 19.31.31104.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.31.31103/bin/Hostx64/x64/cl.exe - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- The CXX compiler identification is MSVC 19.31.31104.0\n",
      "  CMake Warning (dev) at C:/Users/fai-w/AppData/Local/Temp/pip-build-env-3q078nc8/overlay/Lib/site-packages/cmake/data/share/cmake-3.26/Modules/CMakeDetermineCXXCompiler.cmake:168 (if):\n",
      "    Policy CMP0054 is not set: Only interpret if() arguments as variables or\n",
      "    keywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy\n",
      "    details.  Use the cmake_policy command to set the policy and suppress this\n",
      "    warning.\n",
      "  \n",
      "    Quoted variables like \"MSVC\" will no longer be dereferenced when the policy\n",
      "    is set to NEW.  Since the policy is not set the OLD behavior will be used.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:4 (ENABLE_LANGUAGE)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \n",
      "  CMake Warning (dev) at C:/Users/fai-w/AppData/Local/Temp/pip-build-env-3q078nc8/overlay/Lib/site-packages/cmake/data/share/cmake-3.26/Modules/CMakeDetermineCXXCompiler.cmake:189 (elseif):\n",
      "    Policy CMP0054 is not set: Only interpret if() arguments as variables or\n",
      "    keywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy\n",
      "    details.  Use the cmake_policy command to set the policy and suppress this\n",
      "    warning.\n",
      "  \n",
      "    Quoted variables like \"MSVC\" will no longer be dereferenced when the policy\n",
      "    is set to NEW.  Since the policy is not set the OLD behavior will be used.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:4 (ENABLE_LANGUAGE)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.31.31103/bin/Hostx64/x64/cl.exe - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Configuring done (5.3s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/_cmake_test_compile/build\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Visual Studio 17 2022 x64 v143' generator - success\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  Configuring Project\n",
      "    Working directory:\n",
      "      C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\n",
      "    Command:\n",
      "      'C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-build-env-3q078nc8\\overlay\\Lib\\site-packages\\cmake\\data\\bin/cmake.exe' 'C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\opencv' -G 'Visual Studio 17 2022' '-DCMAKE_INSTALL_PREFIX:PATH=C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-install' -DPYTHON_VERSION_STRING:STRING=3.6.10 -DSKBUILD:INTERNAL=TRUE '-DCMAKE_MODULE_PATH:PATH=C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-build-env-3q078nc8\\overlay\\Lib\\site-packages\\skbuild\\resources\\cmake' '-DPYTHON_EXECUTABLE:PATH=D:\\anacnda3\\envs\\cudaenvOld\\python.exe' '-DPYTHON_INCLUDE_DIR:PATH=D:\\anacnda3\\envs\\cudaenvOld\\Include' '-DPYTHON_LIBRARY:PATH=D:\\anacnda3\\envs\\cudaenvOld\\libs\\python36.lib' '-DPython_EXECUTABLE:PATH=D:\\anacnda3\\envs\\cudaenvOld\\python.exe' '-DPython_ROOT_DIR:PATH=D:\\anacnda3\\envs\\cudaenvOld' '-DPython_INCLUDE_DIR:PATH=D:\\anacnda3\\envs\\cudaenvOld\\Include' -DPython_FIND_REGISTRY:STRING=NEVER '-DPython_NumPy_INCLUDE_DIRS:PATH=C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-build-env-3q078nc8\\overlay\\Lib\\site-packages\\numpy\\core\\include' '-DPython3_EXECUTABLE:PATH=D:\\anacnda3\\envs\\cudaenvOld\\python.exe' '-DPython3_ROOT_DIR:PATH=D:\\anacnda3\\envs\\cudaenvOld' '-DPython3_INCLUDE_DIR:PATH=D:\\anacnda3\\envs\\cudaenvOld\\Include' -DPython3_FIND_REGISTRY:STRING=NEVER '-DPython3_NumPy_INCLUDE_DIRS:PATH=C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-build-env-3q078nc8\\overlay\\Lib\\site-packages\\numpy\\core\\include' -T v143 -A x64 '-DPYTHON3_EXECUTABLE=D:\\anacnda3\\envs\\cudaenvOld\\python.exe' -DPYTHON3_INCLUDE_DIR=D:/anacnda3/envs/cudaenvOld/Include -DPYTHON3_LIBRARY=D:/anacnda3/envs/cudaenvOld/libs/python36.lib -DBUILD_opencv_python3=ON -DBUILD_opencv_python2=OFF -DBUILD_opencv_java=OFF -DOPENCV_PYTHON3_INSTALL_PATH=python -DINSTALL_CREATE_DISTRIB=ON -DBUILD_opencv_apps=OFF -DBUILD_opencv_freetype=OFF -DBUILD_SHARED_LIBS=OFF -DBUILD_TESTS=OFF -DBUILD_PERF_TESTS=OFF -DBUILD_DOCS=OFF -DPYTHON3_LIMITED_API=ON -DBUILD_OPENEXR=ON -DCMAKE_GENERATOR_PLATFORM=x64 -DCMAKE_BUILD_TYPE:STRING=Release\n",
      "  \n",
      "  -- Selecting Windows SDK version 10.0.19041.0 to target Windows 10.0.19045.\n",
      "  -- The CXX compiler identification is MSVC 19.31.31104.0\n",
      "  -- The C compiler identification is MSVC 19.31.31104.0\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.31.31103/bin/Hostx64/x64/cl.exe - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.31.31103/bin/Hostx64/x64/cl.exe - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- ocv_init_download: OpenCV source tree is not fetched as git repository. 3rdparty resources will be downloaded from github.com by default.\n",
      "  -- Detected processor: AMD64\n",
      "  -- Found PythonInterp: D:/anacnda3/envs/cudaenvOld/python.exe (found suitable version \"3.6.10\", minimum required is \"2.7\")\n",
      "  CMake Warning at cmake/OpenCVDetectPython.cmake:81 (message):\n",
      "    CMake's 'find_host_package(PythonInterp 2.7)' found wrong Python version:\n",
      "  \n",
      "    PYTHON_EXECUTABLE=D:/anacnda3/envs/cudaenvOld/python.exe\n",
      "  \n",
      "    PYTHON_VERSION_STRING=3.6.10\n",
      "  \n",
      "    Consider providing the 'PYTHON2_EXECUTABLE' variable via CMake command line\n",
      "    or environment variables\n",
      "  \n",
      "  Call Stack (most recent call first):\n",
      "    cmake/OpenCVDetectPython.cmake:271 (find_python)\n",
      "    CMakeLists.txt:643 (include)\n",
      "  \n",
      "  \n",
      "  -- Could NOT find Python2 (missing: Python2_EXECUTABLE Interpreter)\n",
      "      Reason given by package:\n",
      "          Interpreter: Wrong major version for the interpreter \"D:/anacnda3/python.exe\"\n",
      "  \n",
      "  -- Found PythonInterp: D:\\anacnda3\\envs\\cudaenvOld\\python.exe (found suitable version \"3.6.10\", minimum required is \"3.2\")\n",
      "  -- Found PythonLibs: D:/anacnda3/envs/cudaenvOld/libs/python36.lib (found suitable exact version \"3.6.10\")\n",
      "  -- Performing Test HAVE_CXX_FP:PRECISE\n",
      "  -- Performing Test HAVE_CXX_FP:PRECISE - Success\n",
      "  -- Performing Test HAVE_C_FP:PRECISE\n",
      "  -- Performing Test HAVE_C_FP:PRECISE - Success\n",
      "  -- Performing Test HAVE_CPU_SSE3_SUPPORT (check file: cmake/checks/cpu_sse3.cpp)\n",
      "  -- Performing Test HAVE_CPU_SSE3_SUPPORT - Success\n",
      "  -- Performing Test HAVE_CPU_SSSE3_SUPPORT (check file: cmake/checks/cpu_ssse3.cpp)\n",
      "  -- Performing Test HAVE_CPU_SSSE3_SUPPORT - Success\n",
      "  -- Performing Test HAVE_CPU_SSE4_1_SUPPORT (check file: cmake/checks/cpu_sse41.cpp)\n",
      "  -- Performing Test HAVE_CPU_SSE4_1_SUPPORT - Success\n",
      "  -- Performing Test HAVE_CPU_POPCNT_SUPPORT (check file: cmake/checks/cpu_popcnt.cpp)\n",
      "  -- Performing Test HAVE_CPU_POPCNT_SUPPORT - Success\n",
      "  -- Performing Test HAVE_CPU_SSE4_2_SUPPORT (check file: cmake/checks/cpu_sse42.cpp)\n",
      "  -- Performing Test HAVE_CPU_SSE4_2_SUPPORT - Success\n",
      "  -- Performing Test HAVE_CXX_ARCH:AVX (check file: cmake/checks/cpu_fp16.cpp)\n",
      "  -- Performing Test HAVE_CXX_ARCH:AVX - Success\n",
      "  -- Performing Test HAVE_CXX_ARCH:AVX2 (check file: cmake/checks/cpu_avx2.cpp)\n",
      "  -- Performing Test HAVE_CXX_ARCH:AVX2 - Success\n",
      "  -- Performing Test HAVE_CXX_ARCH:AVX512 (check file: cmake/checks/cpu_avx512.cpp)\n",
      "  -- Performing Test HAVE_CXX_ARCH:AVX512 - Success\n",
      "  -- Performing Test HAVE_CPU_BASELINE_FLAGS\n",
      "  -- Performing Test HAVE_CPU_BASELINE_FLAGS - Success\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_SSE4_1\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_SSE4_1 - Success\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_SSE4_2\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_SSE4_2 - Success\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_FP16\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_FP16 - Success\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX - Success\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX2\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX2 - Success\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX512_SKX\n",
      "  -- Performing Test HAVE_CPU_DISPATCH_FLAGS_AVX512_SKX - Success\n",
      "  -- Performing Test HAVE_CXX_W15240\n",
      "  -- Performing Test HAVE_CXX_W15240 - Success\n",
      "  -- Performing Test HAVE_C_W15240\n",
      "  -- Performing Test HAVE_C_W15240 - Success\n",
      "  -- Looking for malloc.h\n",
      "  -- Looking for malloc.h - not found\n",
      "  -- Looking for fseeko\n",
      "  -- Looking for fseeko - not found\n",
      "  -- Looking for sys/types.h\n",
      "  -- Looking for sys/types.h - not found\n",
      "  -- Looking for stdint.h\n",
      "  -- Looking for stdint.h - not found\n",
      "  -- Looking for stddef.h\n",
      "  -- Looking for stddef.h - not found\n",
      "  -- Check size of off64_t\n",
      "  -- Check size of off64_t - failed\n",
      "  -- libjpeg-turbo: VERSION = 2.1.3, BUILD = opencv-4.7.0-libjpeg-turbo\n",
      "  -- Check size of size_t\n",
      "  -- Check size of size_t - failed\n",
      "  -- Check size of unsigned long\n",
      "  -- Check size of unsigned long - failed\n",
      "  -- Looking for include file intrin.h\n",
      "  -- Looking for include file intrin.h - not found\n",
      "  -- Looking for a ASM_NASM compiler\n",
      "  -- Looking for a ASM_NASM compiler - NOTFOUND\n",
      "  -- libjpeg-turbo(SIMD): SIMD extensions disabled: could not find NASM compiler.  Performance will suffer.\n",
      "  -- Looking for assert.h\n",
      "  -- Looking for assert.h - not found\n",
      "  -- Looking for fcntl.h\n",
      "  -- Looking for fcntl.h - not found\n",
      "  -- Looking for inttypes.h\n",
      "  -- Looking for inttypes.h - not found\n",
      "  -- Looking for io.h\n",
      "  -- Looking for io.h - not found\n",
      "  -- Looking for limits.h\n",
      "  -- Looking for limits.h - not found\n",
      "  -- Looking for memory.h\n",
      "  -- Looking for memory.h - not found\n",
      "  -- Looking for search.h\n",
      "  -- Looking for search.h - not found\n",
      "  -- Looking for string.h\n",
      "  -- Looking for string.h - not found\n",
      "  -- Performing Test C_HAS_inline\n",
      "  -- Performing Test C_HAS_inline - Failed\n",
      "  -- Performing Test C_HAS___inline__\n",
      "  -- Performing Test C_HAS___inline__ - Failed\n",
      "  -- Performing Test C_HAS___inline\n",
      "  -- Performing Test C_HAS___inline - Failed\n",
      "  -- Check size of signed short\n",
      "  -- Check size of signed short - failed\n",
      "  -- Check size of unsigned short\n",
      "  -- Check size of unsigned short - failed\n",
      "  -- Check size of signed int\n",
      "  -- Check size of signed int - failed\n",
      "  -- Check size of unsigned int\n",
      "  -- Check size of unsigned int - failed\n",
      "  -- Check size of signed long\n",
      "  -- Check size of signed long - failed\n",
      "  -- Check size of signed long long\n",
      "  -- Check size of signed long long - failed\n",
      "  -- Check size of unsigned long long\n",
      "  -- Check size of unsigned long long - failed\n",
      "  -- Check size of unsigned char *\n",
      "  -- Check size of unsigned char * - failed\n",
      "  -- Check size of ptrdiff_t\n",
      "  -- Check size of ptrdiff_t - failed\n",
      "  -- Looking for memmove\n",
      "  -- Looking for memmove - not found\n",
      "  -- Looking for setmode\n",
      "  -- Looking for setmode - not found\n",
      "  -- Looking for strcasecmp\n",
      "  -- Looking for strcasecmp - not found\n",
      "  -- Looking for strchr\n",
      "  -- Looking for strchr - not found\n",
      "  -- Looking for strrchr\n",
      "  -- Looking for strrchr - not found\n",
      "  -- Looking for strstr\n",
      "  -- Looking for strstr - not found\n",
      "  -- Looking for strtol\n",
      "  -- Looking for strtol - not found\n",
      "  -- Looking for strtol\n",
      "  -- Looking for strtol - not found\n",
      "  -- Looking for strtoull\n",
      "  -- Looking for strtoull - not found\n",
      "  -- Looking for lfind\n",
      "  -- Looking for lfind - not found\n",
      "  -- Performing Test HAVE_SNPRINTF\n",
      "  -- Performing Test HAVE_SNPRINTF - Failed\n",
      "  -- Performing Test HAVE_C_STD_C99\n",
      "  -- Performing Test HAVE_C_STD_C99 - Failed\n",
      "  -- Could NOT find OpenJPEG (minimal suitable version: 2.0, recommended version >= 2.3.1). OpenJPEG will be built from sources\n",
      "  -- OpenJPEG: VERSION = 2.4.0, BUILD = opencv-4.7.0-openjp2-2.4.0\n",
      "  -- The file 'string.h' is mandatory for OpenJPEG build, but not found on your system\n",
      "  -- OpenJPEG libraries can't be built from sources. System requirements are not fulfilled.\n",
      "  -- IPPICV: Downloading ippicv_2020_win_intel64_20191018_general.zip from https://raw.githubusercontent.com/opencv/opencv_3rdparty/a56b6ac6f030c312b2dce17430eef13aed9af274/ippicv/ippicv_2020_win_intel64_20191018_general.zip\n",
      "  -- found Intel IPP (ICV version): 2020.0.0 [2020.0.0 Gold]\n",
      "  -- at: C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/_skbuild/win-amd64-3.6/cmake-build/3rdparty/ippicv/ippicv_win/icv\n",
      "  -- found Intel IPP Integration Wrappers sources: 2020.0.0\n",
      "  -- at: C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/_skbuild/win-amd64-3.6/cmake-build/3rdparty/ippicv/ippicv_win/iw\n",
      "  -- Could not find OpenBLAS include. Turning OpenBLAS_FOUND off\n",
      "  -- Could not find OpenBLAS lib. Turning OpenBLAS_FOUND off\n",
      "  -- Looking for sgemm_\n",
      "  -- Looking for sgemm_ - not found\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
      "  -- Looking for pthread_create in pthreads\n",
      "  -- Looking for pthread_create in pthreads - not found\n",
      "  -- Looking for pthread_create in pthread\n",
      "  -- Looking for pthread_create in pthread - not found\n",
      "  -- Found Threads: TRUE\n",
      "  -- Could NOT find BLAS (missing: BLAS_LIBRARIES)\n",
      "  -- Could NOT find LAPACK (missing: LAPACK_LIBRARIES)\n",
      "      Reason given by package: LAPACK could not be found because dependency BLAS could not be found.\n",
      "  \n",
      "  -- Found JNI: C:/Program Files/Microsoft/jdk-11.0.12.7-hotspot/include  found components: AWT JVM\n",
      "  -- VTK is not found. Please set -DVTK_DIR in CMake to VTK build directory, or to VTK install subdirectory with VTKConfig.cmake file\n",
      "  -- ADE: Downloading v0.1.2a.zip from https://github.com/opencv/ade/archive/v0.1.2a.zip\n",
      "  -- FFMPEG: Downloading opencv_videoio_ffmpeg.dll from https://raw.githubusercontent.com/opencv/opencv_3rdparty/7dd0d4f1d6fe75f05f3d3b5e38cbc96c1a2d2809/ffmpeg/opencv_videoio_ffmpeg.dll\n",
      "  -- FFMPEG: Downloading opencv_videoio_ffmpeg_64.dll from https://raw.githubusercontent.com/opencv/opencv_3rdparty/7dd0d4f1d6fe75f05f3d3b5e38cbc96c1a2d2809/ffmpeg/opencv_videoio_ffmpeg_64.dll\n",
      "  -- FFMPEG: Downloading ffmpeg_version.cmake from https://raw.githubusercontent.com/opencv/opencv_3rdparty/7dd0d4f1d6fe75f05f3d3b5e38cbc96c1a2d2809/ffmpeg/ffmpeg_version.cmake\n",
      "  -- Looking for mfapi.h\n",
      "  -- Looking for mfapi.h - not found\n",
      "  -- Looking for vidcap.h\n",
      "  -- Looking for vidcap.h - not found\n",
      "  -- Could not find mfapi.h. Turning HAVE_OBSENSOR OFF\n",
      "  -- Could not find vidcap.h. Turning HAVE_OBSENSOR OFF\n",
      "  -- Allocator metrics storage type: 'long long'\n",
      "  -- Excluding from source files list: modules/imgproc/src/imgwarp.lasx.cpp\n",
      "  -- Excluding from source files list: modules/imgproc/src/resize.lasx.cpp\n",
      "  -- Registering hook 'INIT_MODULE_SOURCES_opencv_dnn': C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/dnn/cmake/hooks/INIT_MODULE_SOURCES_opencv_dnn.cmake\n",
      "  -- opencv_dnn: filter out cuda4dnn source code\n",
      "  -- Excluding from source files list: <BUILD>/modules/dnn/layers/layers_common.rvv.cpp\n",
      "  -- Excluding from source files list: <BUILD>/modules/dnn/layers/layers_common.lasx.cpp\n",
      "  -- Excluding from source files list: <BUILD>/modules/dnn/int8layers/layers_common.lasx.cpp\n",
      "  -- imgcodecs: Jasper codec is disabled in runtime. Details: https://github.com/opencv/opencv/issues/14058\n",
      "  -- imgcodecs: OpenEXR codec is disabled in runtime. Details: https://github.com/opencv/opencv/issues/21326\n",
      "  -- highgui: using builtin backend: WIN32UI\n",
      "  -- Found 'misc' Python modules from C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/python/package/extra_modules\n",
      "  -- Found 'mat_wrapper;utils' Python modules from C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/core/misc/python/package\n",
      "  -- Found 'gapi' Python modules from C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/gapi/misc/python/package\n",
      "  -- Found 'misc' Python modules from C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/python/package/extra_modules\n",
      "  -- Found 'mat_wrapper;utils' Python modules from C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/core/misc/python/package\n",
      "  -- Found 'gapi' Python modules from C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/gapi/misc/python/package\n",
      "  --\n",
      "  -- General configuration for OpenCV 4.7.0 =====================================\n",
      "  --   Version control:               unknown\n",
      "  --\n",
      "  --   Platform:\n",
      "  --     Timestamp:                   2023-06-15T18:59:26Z\n",
      "  --     Host:                        Windows 10.0.19045 AMD64\n",
      "  --     CMake:                       3.26.4\n",
      "  --     CMake generator:             Visual Studio 17 2022\n",
      "  --     CMake build tool:            C:/Program Files/Microsoft Visual Studio/2022/Community/MSBuild/Current/Bin/amd64/MSBuild.exe\n",
      "  --     MSVC:                        1931\n",
      "  --     Configuration:               Debug Release\n",
      "  --\n",
      "  --   CPU/HW features:\n",
      "  --     Baseline:                    SSE SSE2 SSE3\n",
      "  --       requested:                 SSE3\n",
      "  --     Dispatched code generation:  SSE4_1 SSE4_2 FP16 AVX AVX2 AVX512_SKX\n",
      "  --       requested:                 SSE4_1 SSE4_2 AVX FP16 AVX2 AVX512_SKX\n",
      "  --       SSE4_1 (16 files):         + SSSE3 SSE4_1\n",
      "  --       SSE4_2 (1 files):          + SSSE3 SSE4_1 POPCNT SSE4_2\n",
      "  --       FP16 (0 files):            + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 AVX\n",
      "  --       AVX (4 files):             + SSSE3 SSE4_1 POPCNT SSE4_2 AVX\n",
      "  --       AVX2 (32 files):           + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2\n",
      "  --       AVX512_SKX (5 files):      + SSSE3 SSE4_1 POPCNT SSE4_2 FP16 FMA3 AVX AVX2 AVX_512F AVX512_COMMON AVX512_SKX\n",
      "  --\n",
      "  --   C/C++:\n",
      "  --     Built as dynamic libs?:      NO\n",
      "  --     C++ standard:                11\n",
      "  --     C++ Compiler:                C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.31.31103/bin/Hostx64/x64/cl.exe  (ver 19.31.31104.0)\n",
      "  --     C++ flags (Release):         /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819 /MP  /MT /O2 /Ob2 /DNDEBUG\n",
      "  --     C++ flags (Debug):           /DWIN32 /D_WINDOWS /W4 /GR  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /EHa /wd4127 /wd4251 /wd4324 /wd4275 /wd4512 /wd4589 /wd4819 /MP  /MTd /Zi /Ob0 /Od /RTC1\n",
      "  --     C Compiler:                  C:/Program Files/Microsoft Visual Studio/2022/Community/VC/Tools/MSVC/14.31.31103/bin/Hostx64/x64/cl.exe\n",
      "  --     C flags (Release):           /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP   /MT /O2 /Ob2 /DNDEBUG\n",
      "  --     C flags (Debug):             /DWIN32 /D_WINDOWS /W3  /D _CRT_SECURE_NO_DEPRECATE /D _CRT_NONSTDC_NO_DEPRECATE /D _SCL_SECURE_NO_WARNINGS /Gy /bigobj /Oi  /fp:precise     /MP /MTd /Zi /Ob0 /Od /RTC1\n",
      "  --     Linker flags (Release):      /machine:x64  /NODEFAULTLIB:atlthunk.lib /INCREMENTAL:NO  /NODEFAULTLIB:libcmtd.lib /NODEFAULTLIB:libcpmtd.lib /NODEFAULTLIB:msvcrtd.lib\n",
      "  --     Linker flags (Debug):        /machine:x64  /NODEFAULTLIB:atlthunk.lib /debug /INCREMENTAL  /NODEFAULTLIB:libcmt.lib /NODEFAULTLIB:libcpmt.lib /NODEFAULTLIB:msvcrt.lib\n",
      "  --     ccache:                      NO\n",
      "  --     Precompiled headers:         YES\n",
      "  --     Extra dependencies:          wsock32 comctl32 gdi32 ole32 setupapi ws2_32\n",
      "  --     3rdparty dependencies:       libprotobuf ade ittnotify libjpeg-turbo libwebp libpng libtiff libjasper IlmImf zlib quirc ippiw ippicv\n",
      "  --\n",
      "  --   OpenCV modules:\n",
      "  --     To be built:                 calib3d core dnn features2d flann gapi highgui imgcodecs imgproc ml objdetect photo python3 stitching video videoio\n",
      "  --     Disabled:                    world\n",
      "  --     Disabled by dependency:      -\n",
      "  --     Unavailable:                 java python2 ts\n",
      "  --     Applications:                -\n",
      "  --     Documentation:               NO\n",
      "  --     Non-free algorithms:         NO\n",
      "  --\n",
      "  --   Windows RT support:            NO\n",
      "  --\n",
      "  --   GUI:                           WIN32UI\n",
      "  --     Win32 UI:                    YES\n",
      "  --     VTK support:                 NO\n",
      "  --\n",
      "  --   Media I/O:\n",
      "  --     ZLib:                        build (ver 1.2.13)\n",
      "  --     JPEG:                        build-libjpeg-turbo (ver 2.1.3-62)\n",
      "  --       SIMD Support Request:      YES\n",
      "  --       SIMD Support:              NO\n",
      "  --     WEBP:                        build (ver encoder: 0x020f)\n",
      "  --     PNG:                         build (ver 1.6.37)\n",
      "  --     TIFF:                        build (ver 42 - 4.2.0)\n",
      "  --     JPEG 2000:                   build Jasper (ver 1.900.1)\n",
      "  --     OpenEXR:                     build (ver 2.3.0)\n",
      "  --     HDR:                         YES\n",
      "  --     SUNRASTER:                   YES\n",
      "  --     PXM:                         YES\n",
      "  --     PFM:                         YES\n",
      "  --\n",
      "  --   Video I/O:\n",
      "  --     DC1394:                      NO\n",
      "  --     FFMPEG:                      YES (prebuilt binaries)\n",
      "  --       avcodec:                   YES (58.134.100)\n",
      "  --       avformat:                  YES (58.76.100)\n",
      "  --       avutil:                    YES (56.70.100)\n",
      "  --       swscale:                   YES (5.9.100)\n",
      "  --       avresample:                YES (4.0.0)\n",
      "  --     GStreamer:                   NO\n",
      "  --     DirectShow:                  YES\n",
      "  --     Media Foundation:            NO\n",
      "  --       DXVA:                      NO\n",
      "  --\n",
      "  --   Parallel framework:            Concurrency\n",
      "  --\n",
      "  --   Trace:                         YES (with Intel ITT)\n",
      "  --\n",
      "  --   Other third-party libraries:\n",
      "  --     Intel IPP:                   2020.0.0 Gold [2020.0.0]\n",
      "  --            at:                   C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/_skbuild/win-amd64-3.6/cmake-build/3rdparty/ippicv/ippicv_win/icv\n",
      "  --     Intel IPP IW:                sources (2020.0.0)\n",
      "  --               at:                C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/_skbuild/win-amd64-3.6/cmake-build/3rdparty/ippicv/ippicv_win/iw\n",
      "  --     Lapack:                      NO\n",
      "  --     Eigen:                       NO\n",
      "  --     Custom HAL:                  NO\n",
      "  --     Protobuf:                    build (3.19.1)\n",
      "  --\n",
      "  --   OpenCL:                        YES (NVD3D11)\n",
      "  --     Include path:                C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/3rdparty/include/opencl/1.2\n",
      "  --     Link libraries:              Dynamic load\n",
      "  --\n",
      "  --   Python 3:\n",
      "  --     Interpreter:                 D:/anacnda3/envs/cudaenvOld/python.exe (ver 3.6.10)\n",
      "  --     Libraries:                   D:/anacnda3/envs/cudaenvOld/libs/python36.lib (ver 3.6.10)\n",
      "  --     numpy:                       C:/Users/fai-w/AppData/Local/Temp/pip-build-env-3q078nc8/overlay/Lib/site-packages/numpy/core/include (ver 1.13.3)\n",
      "  --     install path:                python/cv2/python-3\n",
      "  --\n",
      "  --   Python (for build):            D:/anacnda3/envs/cudaenvOld/python.exe\n",
      "  --\n",
      "  --   Java:\n",
      "  --     ant:                         NO\n",
      "  --     JNI:                         C:/Program Files/Microsoft/jdk-11.0.12.7-hotspot/include C:/Program Files/Microsoft/jdk-11.0.12.7-hotspot/include/win32 C:/Program Files/Microsoft/jdk-11.0.12.7-hotspot/include\n",
      "  --     Java wrappers:               NO\n",
      "  --     Java tests:                  NO\n",
      "  --\n",
      "  --   Install to:                    C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/_skbuild/win-amd64-3.6/cmake-install\n",
      "  -- -----------------------------------------------------------------\n",
      "  --\n",
      "  -- Configuring done (281.0s)\n",
      "  -- Generating done (2.8s)\n",
      "  -- Build files have been written to: C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/_skbuild/win-amd64-3.6/cmake-build\n",
      "  Microsoft (R) Build Engine version 17.1.0+ae57d105c for .NET Framework\n",
      "  Copyright (C) Microsoft Corporation. All rights reserved.\n",
      "  \n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\ZERO_CHECK.vcxproj]\n",
      "    Checking Build System\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\3rdparty\\libjpeg-turbo\\jsimd.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/3rdparty/libjpeg-turbo/CMakeLists.txt\n",
      "    jsimd_none.c\n",
      "    jsimd.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\3rdparty\\libjpeg-turbo\\jsimd.dir\\Release\\jsimd.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\calib3d\\opencv_calib3d_AVX2.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/calib3d/CMakeLists.txt\n",
      "    undistort.avx2.cpp\n",
      "    opencv_calib3d_AVX2.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\calib3d\\opencv_calib3d_AVX2.dir\\Release\\opencv_calib3d_AVX2.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\core\\opencv_core_AVX.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/core/CMakeLists.txt\n",
      "    mathfuncs_core.avx.cpp\n",
      "    opencv_core_AVX.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\core\\opencv_core_AVX.dir\\Release\\opencv_core_AVX.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\core\\opencv_core_AVX2.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/core/CMakeLists.txt\n",
      "    mathfuncs_core.avx2.cpp\n",
      "    stat.avx2.cpp\n",
      "    arithm.avx2.cpp\n",
      "    convert.avx2.cpp\n",
      "    convert_scale.avx2.cpp\n",
      "    count_non_zero.avx2.cpp\n",
      "    matmul.avx2.cpp\n",
      "    mean.avx2.cpp\n",
      "    merge.avx2.cpp\n",
      "    split.avx2.cpp\n",
      "    sum.avx2.cpp\n",
      "    opencv_core_AVX2.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\core\\opencv_core_AVX2.dir\\Release\\opencv_core_AVX2.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\core\\opencv_core_AVX512_SKX.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/core/CMakeLists.txt\n",
      "    matmul.avx512_skx.cpp\n",
      "    opencv_core_AVX512_SKX.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\core\\opencv_core_AVX512_SKX.dir\\Release\\opencv_core_AVX512_SKX.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\core\\opencv_core_SSE4_1.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/core/CMakeLists.txt\n",
      "    arithm.sse4_1.cpp\n",
      "    matmul.sse4_1.cpp\n",
      "    opencv_core_SSE4_1.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\core\\opencv_core_SSE4_1.dir\\Release\\opencv_core_SSE4_1.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\core\\opencv_core_SSE4_2.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/core/CMakeLists.txt\n",
      "    stat.sse4_2.cpp\n",
      "    opencv_core_SSE4_2.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\core\\opencv_core_SSE4_2.dir\\Release\\opencv_core_SSE4_2.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\dnn\\opencv_dnn_AVX.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/dnn/CMakeLists.txt\n",
      "    layers_common.avx.cpp\n",
      "    opencv_dnn_AVX.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\dnn\\opencv_dnn_AVX.dir\\Release\\opencv_dnn_AVX.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\dnn\\opencv_dnn_AVX2.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/dnn/CMakeLists.txt\n",
      "    fast_convolution.avx2.cpp\n",
      "    layers_common.avx2.cpp\n",
      "    layers_common.avx2.cpp\n",
      "    opencv_dnn_AVX2.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\dnn\\opencv_dnn_AVX2.dir\\Release\\opencv_dnn_AVX2.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\dnn\\opencv_dnn_AVX512_SKX.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/dnn/CMakeLists.txt\n",
      "    layers_common.avx512_skx.cpp\n",
      "    layers_common.avx512_skx.cpp\n",
      "    opencv_dnn_AVX512_SKX.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\dnn\\opencv_dnn_AVX512_SKX.dir\\Release\\opencv_dnn_AVX512_SKX.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\opencv_dnn_plugins.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/CMakeLists.txt\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\features2d\\opencv_features2d_AVX2.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/features2d/CMakeLists.txt\n",
      "    fast.avx2.cpp\n",
      "    sift.avx2.cpp\n",
      "    opencv_features2d_AVX2.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\features2d\\opencv_features2d_AVX2.dir\\Release\\opencv_features2d_AVX2.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\features2d\\opencv_features2d_AVX512_SKX.vcxproj]\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(382,5): error MSB3491: Could not write lines to file \"opencv_features2d_AVX512_SKX.dir\\Release\\opencv_f.32BCFEB7.tlog\\opencv_features2d_AVX512_SKX.lastbuildstate\". Path: opencv_features2d_AVX512_SKX.dir\\Release\\opencv_f.32BCFEB7.tlog\\opencv_features2d_AVX512_SKX.lastbuildstate exceeds the OS max path limit. The fully qualified file name must be less than 260 characters. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\features2d\\opencv_features2d_AVX512_SKX.vcxproj]\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\features2d\\opencv_features2d_SSE4_1.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/features2d/CMakeLists.txt\n",
      "    sift.sse4_1.cpp\n",
      "    opencv_features2d_SSE4_1.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\features2d\\opencv_features2d_SSE4_1.dir\\Release\\opencv_features2d_SSE4_1.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\gapi\\opencv_gapi_AVX2.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/gapi/CMakeLists.txt\n",
      "    gfluidimgproc_func.avx2.cpp\n",
      "    gfluidcore_func.avx2.cpp\n",
      "    opencv_gapi_AVX2.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\gapi\\opencv_gapi_AVX2.dir\\Release\\opencv_gapi_AVX2.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\gapi\\opencv_gapi_SSE4_1.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/gapi/CMakeLists.txt\n",
      "    gfluidimgproc_func.sse4_1.cpp\n",
      "    gfluidcore_func.sse4_1.cpp\n",
      "    opencv_gapi_SSE4_1.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\gapi\\opencv_gapi_SSE4_1.dir\\Release\\opencv_gapi_SSE4_1.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\opencv_highgui_plugins.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/CMakeLists.txt\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\imgproc\\opencv_imgproc_AVX.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/imgproc/CMakeLists.txt\n",
      "    corner.avx.cpp\n",
      "    accum.avx.cpp\n",
      "    opencv_imgproc_AVX.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\imgproc\\opencv_imgproc_AVX.dir\\Release\\opencv_imgproc_AVX.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\imgproc\\opencv_imgproc_AVX2.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/imgproc/CMakeLists.txt\n",
      "    imgwarp.avx2.cpp\n",
      "    resize.avx2.cpp\n",
      "    accum.avx2.cpp\n",
      "    bilateral_filter.avx2.cpp\n",
      "    box_filter.avx2.cpp\n",
      "    filter.avx2.cpp\n",
      "    color_hsv.avx2.cpp\n",
      "    color_rgb.avx2.cpp\n",
      "    color_yuv.avx2.cpp\n",
      "    median_blur.avx2.cpp\n",
      "    morph.avx2.cpp\n",
      "    smooth.avx2.cpp\n",
      "    sumpixels.avx2.cpp\n",
      "    opencv_imgproc_AVX2.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\imgproc\\opencv_imgproc_AVX2.dir\\Release\\opencv_imgproc_AVX2.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\imgproc\\opencv_imgproc_AVX512_SKX.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/imgproc/CMakeLists.txt\n",
      "    sumpixels.avx512_skx.cpp\n",
      "    opencv_imgproc_AVX512_SKX.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\imgproc\\opencv_imgproc_AVX512_SKX.dir\\Release\\opencv_imgproc_AVX512_SKX.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\imgproc\\opencv_imgproc_SSE4_1.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/imgproc/CMakeLists.txt\n",
      "    imgwarp.sse4_1.cpp\n",
      "    resize.sse4_1.cpp\n",
      "    accum.sse4_1.cpp\n",
      "    box_filter.sse4_1.cpp\n",
      "    filter.sse4_1.cpp\n",
      "    color_hsv.sse4_1.cpp\n",
      "    color_rgb.sse4_1.cpp\n",
      "    color_yuv.sse4_1.cpp\n",
      "    median_blur.sse4_1.cpp\n",
      "    morph.sse4_1.cpp\n",
      "    smooth.sse4_1.cpp\n",
      "    opencv_imgproc_SSE4_1.vcxproj -> C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\imgproc\\opencv_imgproc_SSE4_1.dir\\Release\\opencv_imgproc_SSE4_1.lib\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\python_bindings_generator\\gen_opencv_python_source.vcxproj]\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(382,5): error MSB3491: Could not write lines to file \"x64\\Release\\gen_opencv_python_source\\gen_open.78BB40AD.tlog\\gen_opencv_python_source.lastbuildstate\". Path: x64\\Release\\gen_opencv_python_source\\gen_open.78BB40AD.tlog\\gen_opencv_python_source.lastbuildstate exceeds the OS max path limit. The fully qualified file name must be less than 260 characters. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\python_bindings_generator\\gen_opencv_python_source.vcxproj]\n",
      "  C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\MSBuild\\Microsoft\\VC\\v170\\Microsoft.CppBuild.targets(524,5): warning MSB8029: The Intermediate directory or Output directory cannot reside under the Temporary directory as it could lead to issues with incremental build. [C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\\modules\\.firstpass\\videoio\\opencv_videoio_plugins.vcxproj]\n",
      "    Building Custom Rule C:/Users/fai-w/AppData/Local/Temp/pip-install-6wmj3acn/opencv-python_ea6751d4f37340d6a4f238f786210f45/opencv/modules/videoio/CMakeLists.txt\n",
      "  Traceback (most recent call last):\n",
      "    File \"C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-build-env-3q078nc8\\overlay\\Lib\\site-packages\\skbuild\\setuptools_wrap.py\", line 642, in setup\n",
      "      cmkr.make(make_args, install_target=cmake_install_target, env=env)\n",
      "    File \"C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-build-env-3q078nc8\\overlay\\Lib\\site-packages\\skbuild\\cmaker.py\", line 679, in make\n",
      "      self.make_impl(clargs=clargs, config=config, source_dir=source_dir, install_target=install_target, env=env)\n",
      "    File \"C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-build-env-3q078nc8\\overlay\\Lib\\site-packages\\skbuild\\cmaker.py\", line 711, in make_impl\n",
      "      \"An error occurred while building with CMake.\\n\"\n",
      "  \n",
      "  An error occurred while building with CMake.\n",
      "    Command:\n",
      "      'C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-build-env-3q078nc8\\overlay\\Lib\\site-packages\\cmake\\data\\bin/cmake.exe' --build . --target install --config Release --\n",
      "    Install target:\n",
      "      install\n",
      "    Source directory:\n",
      "      C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\n",
      "    Working directory:\n",
      "      C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-6wmj3acn\\opencv-python_ea6751d4f37340d6a4f238f786210f45\\_skbuild\\win-amd64-3.6\\cmake-build\n",
      "  Please check the install target is valid and see CMake's output for more information.\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for opencv-python\n",
      "ERROR: Could not build wheels for opencv-python, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8334958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python==3.4.13.47\n",
      "  Downloading opencv_python-3.4.13.47-cp36-cp36m-win_amd64.whl (30.9 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from opencv-python==3.4.13.47) (1.17.0)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-3.4.13.47\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install opencv-python==3.4.13.47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8dc3412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (21.3.1)\n",
      "Requirement already satisfied: setuptools in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (59.6.0)\n",
      "Requirement already satisfied: wheel in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (0.37.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5077875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cvzone\n",
      "  Using cached cvzone-1.5.6.tar.gz (12 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: opencv-python in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from cvzone) (3.4.13.47)\n",
      "Requirement already satisfied: numpy in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from cvzone) (1.17.0)\n",
      "Building wheels for collected packages: cvzone\n",
      "  Building wheel for cvzone (setup.py): started\n",
      "  Building wheel for cvzone (setup.py): finished with status 'done'\n",
      "  Created wheel for cvzone: filename=cvzone-1.5.6-py3-none-any.whl size=18768 sha256=359048ea5131eb9954c272a3a028ffe67fce02eae6b602a70cddc99fa9aa6943\n",
      "  Stored in directory: c:\\users\\fai-w\\appdata\\local\\pip\\cache\\wheels\\53\\72\\28\\0b6a96b2cbddac206aa52e2a860967e3297b741d1d1a4567f5\n",
      "Successfully built cvzone\n",
      "Installing collected packages: cvzone\n",
      "Successfully installed cvzone-1.5.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install cvzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21cacfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yolo\n",
      "  Using cached yolo-0.3.2-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: requests in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from yolo) (2.24.0)\n",
      "Collecting voluptuous\n",
      "  Using cached voluptuous-0.13.1-py3-none-any.whl (29 kB)\n",
      "Collecting boto3\n",
      "  Using cached boto3-1.23.10-py3-none-any.whl (132 kB)\n",
      "Collecting awscli\n",
      "  Using cached awscli-1.24.10-py3-none-any.whl (3.9 MB)\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting keyring==8.7.0\n",
      "  Using cached keyring-8.7-py2.py3-none-any.whl (33 kB)\n",
      "Collecting keyrings.alt\n",
      "  Using cached keyrings.alt-4.1.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: botocore>=1.7.18 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from yolo) (1.26.10)\n",
      "Requirement already satisfied: click in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from yolo) (7.1.2)\n",
      "Requirement already satisfied: ruamel.yaml in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from yolo) (0.15.87)\n",
      "Requirement already satisfied: jinja2 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from yolo) (2.11.2)\n",
      "Collecting docker==3.4.0\n",
      "  Using cached docker-3.4.0-py2.py3-none-any.whl (122 kB)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from docker==3.4.0->yolo) (1.3.1)\n",
      "Collecting pypiwin32==220\n",
      "  Using cached pypiwin32-220-cp36-none-win_amd64.whl (9.0 MB)\n",
      "Collecting docker-pycreds>=0.3.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: six>=1.4.0 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from docker==3.4.0->yolo) (1.15.0)\n",
      "Requirement already satisfied: pywin32-ctypes in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from keyring==8.7.0->yolo) (0.2.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from botocore>=1.7.18->yolo) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from botocore>=1.7.18->yolo) (1.25.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from botocore>=1.7.18->yolo) (2.8.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from requests->yolo) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from requests->yolo) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from requests->yolo) (3.0.4)\n",
      "Requirement already satisfied: docutils<0.17,>=0.10 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from awscli->yolo) (0.16)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from awscli->yolo) (4.7.2)\n",
      "Requirement already satisfied: colorama<0.4.5,>=0.2.5 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from awscli->yolo) (0.4.3)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from awscli->yolo) (0.5.2)\n",
      "Requirement already satisfied: PyYAML<5.5,>=3.10 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from awscli->yolo) (5.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from jinja2->yolo) (1.1.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\anacnda3\\envs\\cudaenvold\\lib\\site-packages (from rsa<4.8,>=3.1.2->awscli->yolo) (0.5.0)\n",
      "Installing collected packages: pypiwin32, docker-pycreds, voluptuous, tabulate, keyrings.alt, keyring, docker, boto3, awscli, yoloNote: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'D:\\\\anacnda3\\\\envs\\\\cudaenvOld\\\\Lib\\\\site-packages\\\\win32\\\\win32api.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##pip install yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9ad7cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Ultralytics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mUltralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Ultralytics'"
     ]
    }
   ],
   "source": [
    "#from Ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd25c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement ultralytics-yolov5 (from versions: none)\n",
      "ERROR: No matching distribution found for ultralytics-yolov5\n"
     ]
    }
   ],
   "source": [
    "#pip install ultralytics-yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e539271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement ultralytics (from versions: none)\n",
      "ERROR: No matching distribution found for ultralytics\n"
     ]
    }
   ],
   "source": [
    "#pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c9dc647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 405 Method Not Allowed',))': /simple/ultralytics/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 405 Method Not Allowed',))': /simple/ultralytics/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 405 Method Not Allowed',))': /simple/ultralytics/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 405 Method Not Allowed',))': /simple/ultralytics/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 405 Method Not Allowed',))': /simple/ultralytics/\n",
      "ERROR: Could not find a version that satisfies the requirement ultralytics (from versions: none)\n",
      "ERROR: No matching distribution found for ultralytics\n"
     ]
    }
   ],
   "source": [
    "#pip install ultralytics --proxy localhost:8892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524dbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c8546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=C:\\Users\\fai-w\\Desktop\\weights\\yolov5n.pt' with new 'model=C:\\Users\\fai-w\\Desktop\\weights\\yolov5nu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#downloading weights n means nano\n",
    "#gives smoother webcam performance\n",
    "#YOU WILL GET ERROR, PUT 'yolov5n.pt' ONLY ON THE PATH\n",
    "model=YOLO(r'C:\\Users\\fai-w\\Desktop\\weights\\yolov5n.pt')\n",
    "#yolov8l.pt -> webcam gets laggy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49483568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to stop mage from closing when preprocessng is complete\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16563931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\fai-w\\Desktop\\download.jpg: 640x640 1 person, 215.5ms\n",
      "Speed: 7.0ms preprocess, 215.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.yolo.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.yolo.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " orig_img: array([[[253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       " \n",
       "        [[253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       " \n",
       "        [[253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       " \n",
       "        [[253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       " \n",
       "        [[253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]]], dtype=uint8)\n",
       " orig_shape: (263, 263)\n",
       " path: 'C:\\\\Users\\\\fai-w\\\\Desktop\\\\download.jpg'\n",
       " probs: None\n",
       " speed: {'preprocess': 6.983280181884766, 'inference': 215.5478000640869, 'postprocess': 2.99072265625}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show results of image then closes image\n",
    "#results=model(r\"C:\\Users\\fai-w\\Desktop\\download.jpg\", show=True)\n",
    "#keeps image open\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "#results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "651d25cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6686) tensor(12.2176) tensor(171.0649) tensor(233.7642)\n"
     ]
    }
   ],
   "source": [
    "#for r in results:\n",
    "#        boxes=r.boxes\n",
    "#        #get Xs and Ys of each box\n",
    "#        for box in boxes:\n",
    "#            x1, y1, x2, y2 = box.xyxy[0]\n",
    "#            print(x1, y1, x2, y2)\n",
    "#IDEA USE BETTER ACCURACY WHEN PEOPLE WANT TO UPLOAD AN IMAGE \n",
    "#BUT IN REALTIME OBJECT DETECTION GET WORSE WEIGHT BUT ADJUST MANUALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00dbbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#colorss=pd.read_csv('D:\\color_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c488dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1298 entries, 0 to 1297\n",
      "Data columns (total 8 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Name                             1298 non-null   object \n",
      " 1   Hex (24 bit)                     1298 non-null   object \n",
      " 2   Red (8 bit)                      1298 non-null   int64  \n",
      " 3   Green (8 bit)                    1298 non-null   int64  \n",
      " 4   Blue (8 bit)                     1298 non-null   int64  \n",
      " 5   Hue (degrees)                    1298 non-null   float64\n",
      " 6   HSL.S (%)                        1298 non-null   float64\n",
      " 7   HSL.L (%), HSV.S (%), HSV.V (%)  1298 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(2)\n",
      "memory usage: 81.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#colorss.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f46251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 129.7ms\n",
      "Speed: 3.0ms preprocess, 129.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(225.4683) tensor(300.3722) tensor(702.3552) tensor(712.5568)\n",
      "tensor(856.3962) tensor(248.8972) tensor(1278.7039) tensor(712.2711)\n",
      "tensor(31.2318) tensor(545.9716) tensor(67.8279) tensor(614.7877)\n",
      "tensor(426.2377) tensor(425.3495) tensor(530.4456) tensor(547.1268)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 refrigerator, 156.6ms\n",
      "Speed: 4.0ms preprocess, 156.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(226.8945) tensor(298.7195) tensor(702.1696) tensor(710.8550)\n",
      "tensor(448.0874) tensor(145.3499) tensor(783.2598) tensor(598.6647)\n",
      "tensor(850.6022) tensor(246.5703) tensor(1278.5491) tensor(711.7491)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 154.6ms\n",
      "Speed: 3.0ms preprocess, 154.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cell phone, 119.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(222.2347) tensor(299.1368) tensor(701.5458) tensor(711.8085)\n",
      "tensor(855.4942) tensor(250.1456) tensor(1278.7231) tensor(713.6913)\n",
      "tensor(34.8644) tensor(544.3397) tensor(67.8036) tensor(614.1307)\n",
      "tensor(215.0822) tensor(299.7560) tensor(701.1444) tensor(710.0035)\n",
      "tensor(849.7513) tensor(249.1707) tensor(1278.4211) tensor(711.3827)\n",
      "tensor(421.0537) tensor(446.1234) tensor(518.7480) tensor(544.4208)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 119.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 129.7ms\n",
      "Speed: 4.0ms preprocess, 129.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(224.2853) tensor(300.1129) tensor(700.3002) tensor(708.7052)\n",
      "tensor(425.3782) tensor(442.4107) tensor(523.7690) tensor(555.2010)\n",
      "tensor(851.1193) tensor(249.2719) tensor(1278.4104) tensor(712.5045)\n",
      "tensor(33.9379) tensor(545.6713) tensor(68.8417) tensor(616.3947)\n",
      "tensor(223.1979) tensor(298.9099) tensor(703.1558) tensor(710.4368)\n",
      "tensor(853.6043) tensor(249.4886) tensor(1278.4407) tensor(711.9479)\n",
      "tensor(306.2278) tensor(435.1788) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 2 cell phones, 136.6ms\n",
      "Speed: 3.0ms preprocess, 136.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(540.1658) tensor(562.3981)\n",
      "tensor(433.7458) tensor(446.5436) tensor(534.6456) tensor(556.7754)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 119.7ms\n",
      "Speed: 3.0ms preprocess, 119.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(849.8809) tensor(251.0026) tensor(1278.4290) tensor(712.0481)\n",
      "tensor(34.4539) tensor(544.0883) tensor(68.4793) tensor(614.9717)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 3 cell phones, 139.6ms\n",
      "Speed: 3.0ms preprocess, 139.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(435.0095) tensor(424.2857) tensor(536.1586) tensor(550.2532)\n",
      "tensor(850.7542) tensor(247.7426) tensor(1278.5149) tensor(712.5057)\n",
      "tensor(214.5534) tensor(297.1790) tensor(704.1097) tensor(710.7653)\n",
      "tensor(318.2729) tensor(393.9282) tensor(474.4841) tensor(516.5991)\n",
      "tensor(317.1161) tensor(389.8832) tensor(539.5583) tensor(548.7974)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bed, 136.6ms\n",
      "Speed: 2.0ms preprocess, 136.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(225.7434) tensor(298.1620) tensor(704.1219) tensor(712.7052)\n",
      "tensor(664.1366) tensor(230.7957) tensor(1280.) tensor(712.9824)\n",
      "tensor(851.3926) tensor(246.7921) tensor(1278.6289) tensor(712.9143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 1 refrigerator, 134.6ms\n",
      "Speed: 3.0ms preprocess, 134.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(219.3898) tensor(300.5244) tensor(704.2349) tensor(711.3921)\n",
      "tensor(853.6322) tensor(246.4565) tensor(1278.5927) tensor(712.9687)\n",
      "tensor(322.8664) tensor(399.2216) tensor(539.2878) tensor(638.0299)\n",
      "tensor(445.9835) tensor(145.7925) tensor(779.9840) tensor(599.2679)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 127.7ms\n",
      "Speed: 2.0ms preprocess, 127.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(853.1694) tensor(249.5828) tensor(1278.6672) tensor(713.2633)\n",
      "tensor(330.7700) tensor(445.0236) tensor(535.9975) tensor(563.1820)\n",
      "tensor(215.7101) tensor(299.5547) tensor(704.3108) tensor(711.3961)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 2 cell phones, 147.6ms\n",
      "Speed: 2.0ms preprocess, 147.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(219.5613) tensor(299.9044) tensor(703.9192) tensor(709.8572)\n",
      "tensor(853.1392) tensor(247.6273) tensor(1278.5439) tensor(712.1965)\n",
      "tensor(337.0881) tensor(396.7391) tensor(533.6428) tensor(537.6892)\n",
      "tensor(338.8453) tensor(454.3183) tensor(533.9958) tensor(528.5305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 refrigerator, 126.7ms\n",
      "Speed: 2.0ms preprocess, 126.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(220.4624) tensor(300.3737) tensor(704.8361) tensor(710.9540)\n",
      "tensor(852.9653) tensor(248.7155) tensor(1278.4167) tensor(711.8639)\n",
      "tensor(446.9648) tensor(145.7717) tensor(779.1834) tensor(600.4851)\n",
      "tensor(34.3678) tensor(544.8412) tensor(69.7678) tensor(616.4916)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 1 refrigerator, 140.6ms\n",
      "Speed: 2.0ms preprocess, 140.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(221.3361) tensor(300.3163) tensor(704.6951) tensor(709.3144)\n",
      "tensor(853.2042) tensor(245.7508) tensor(1278.5311) tensor(712.6576)\n",
      "tensor(446.8983) tensor(145.8138) tensor(780.9227) tensor(599.5223)\n",
      "tensor(432.9241) tensor(449.4343) tensor(534.5793) tensor(554.6842)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 138.6ms\n",
      "Speed: 3.0ms preprocess, 138.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(304.2018) tensor(390.7299) tensor(534.8079) tensor(569.0505)\n",
      "tensor(220.1805) tensor(298.2021) tensor(703.1880) tensor(710.8770)\n",
      "tensor(853.5886) tensor(247.7577) tensor(1278.3967) tensor(712.2668)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 131.6ms\n",
      "Speed: 2.0ms preprocess, 131.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(226.8855) tensor(300.5873) tensor(702.6177) tensor(709.4501)\n",
      "tensor(855.5196) tensor(247.3430) tensor(1278.5532) tensor(713.1881)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 118.7ms\n",
      "Speed: 2.0ms preprocess, 118.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(225.3542) tensor(300.4058) tensor(702.0313) tensor(709.5205)\n",
      "tensor(313.6365) tensor(419.5736) tensor(492.7760) tensor(532.2252)\n",
      "tensor(855.6710) tensor(248.1770) tensor(1278.8024) tensor(711.2303)\n",
      "tensor(853.2607) tensor(246.7241) tensor(1278.6533) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 refrigerator, 135.6ms\n",
      "Speed: 2.0ms preprocess, 135.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(713.0793)\n",
      "tensor(236.4992) tensor(301.2570) tensor(623.1955) tensor(710.8380)\n",
      "tensor(446.3869) tensor(145.4146) tensor(780.4064) tensor(600.6025)\n",
      "tensor(34.8201) tensor(543.8085) tensor(68.1505) tensor(614.0549)\n",
      "tensor(318.3050) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 131.6ms\n",
      "Speed: 2.0ms preprocess, 131.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(394.0090) tensor(539.4368) tensor(554.7514)\n",
      "tensor(216.9997) tensor(299.9171) tensor(701.0073) tensor(710.8898)\n",
      "tensor(855.3537) tensor(248.0056) tensor(1278.6218) tensor(712.8054)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 parking meter, 1 cell phone, 137.6ms\n",
      "Speed: 3.0ms preprocess, 137.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(867.8212) tensor(246.8910) tensor(1278.4711) tensor(712.6918)\n",
      "tensor(220.6514) tensor(300.7287) tensor(620.3885) tensor(711.1591)\n",
      "tensor(219.5878) tensor(301.0436) tensor(702.7421) tensor(710.8729)\n",
      "tensor(446.4863) tensor(398.7070) tensor(537.5638) tensor(560.2225)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 134.6ms\n",
      "Speed: 2.0ms preprocess, 134.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(218.9372) tensor(300.8046) tensor(703.8223) tensor(712.4077)\n",
      "tensor(852.1010) tensor(248.4133) tensor(1278.5430) tensor(712.4755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 119.7ms\n",
      "Speed: 3.0ms preprocess, 119.7ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(228.3311) tensor(296.9475) tensor(704.3109) tensor(710.4268)\n",
      "tensor(854.1736) tensor(247.0403) tensor(1278.6594) tensor(711.5959)\n",
      "tensor(34.4278) tensor(544.3293) tensor(69.6559) tensor(617.8752)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 118.7ms\n",
      "Speed: 2.0ms preprocess, 118.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(238.7532) tensor(276.8250) tensor(713.2990) tensor(712.2515)\n",
      "tensor(856.9260) tensor(248.4726) tensor(1278.6697) tensor(713.3296)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bed, 1 cell phone, 143.6ms\n",
      "Speed: 2.0ms preprocess, 143.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(441.2650) tensor(391.1108) tensor(600.2551) tensor(696.8219)\n",
      "tensor(264.4574) tensor(261.9171) tensor(726.7346) tensor(712.4162)\n",
      "tensor(858.7570) tensor(245.9559) tensor(1278.7125) tensor(713.2714)\n",
      "tensor(726.4121) tensor(239.7024) tensor(1280.) tensor(713.0195)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 126.7ms\n",
      "Speed: 2.0ms preprocess, 126.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cell phone, 125.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(304.8289) tensor(256.7267) tensor(829.9514) tensor(714.7092)\n",
      "tensor(568.6254) tensor(377.8908) tensor(649.2410) tensor(437.2472)\n",
      "tensor(871.2745) tensor(246.5444) tensor(1279.0809) tensor(711.6243)\n",
      "tensor(305.9115) tensor(248.0304) tensor(961.3754) tensor(714.3168)\n",
      "tensor(883.6235) tensor(249.0798) tensor(1278.6138) tensor(711.4447)\n",
      "tensor(306.8409) tensor(250.4227) tensor(761.0421) tensor(716.3467)\n",
      "tensor(551.1323) tensor(375.2886) tensor(631.0862) tensor(431.7423)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 125.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 140.6ms\n",
      "Speed: 2.0ms preprocess, 140.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(329.5398) tensor(243.7947) tensor(1095.2844) tensor(713.5172)\n",
      "tensor(880.7090) tensor(250.2739) tensor(1278.9937) tensor(711.3072)\n",
      "tensor(31.4602) tensor(544.4470) tensor(69.4898) tensor(619.7249)\n",
      "tensor(512.6173) tensor(447.4847) tensor(654.1847) tensor(634.9307)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 138.6ms\n",
      "Speed: 2.0ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 128.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(336.0222) tensor(243.8189) tensor(981.3784) tensor(714.6929)\n",
      "tensor(871.2296) tensor(248.2778) tensor(1278.7354) tensor(712.3305)\n",
      "tensor(341.1224) tensor(238.8387) tensor(1212.0576) tensor(714.1661)\n",
      "tensor(874.5225) tensor(252.0659) tensor(1278.9946) tensor(710.6381)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 128.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 129.7ms\n",
      "Speed: 3.0ms preprocess, 129.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 laptop, 124.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(319.4326) tensor(240.5131) tensor(1239.0984) tensor(711.6564)\n",
      "tensor(334.1709) tensor(255.4874) tensor(1251.8940) tensor(714.7084)\n",
      "tensor(969.7126) tensor(350.6243) tensor(1237.6121) tensor(568.8499)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 124.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 144.6ms\n",
      "Speed: 2.0ms preprocess, 144.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(355.7092) tensor(257.8517) tensor(1188.4998) tensor(714.7710)\n",
      "tensor(348.0328) tensor(256.1705) tensor(912.5803) tensor(714.1893)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 133.6ms\n",
      "Speed: 3.0ms preprocess, 133.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 (no detections), 123.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.2536) tensor(545.2764) tensor(68.8215) tensor(617.1873)\n",
      "tensor(967.9661) tensor(248.6656) tensor(1279.1926) tensor(709.8154)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 123.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 139.6ms\n",
      "Speed: 2.0ms preprocess, 139.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(933.8558) tensor(250.4703) tensor(1279.2369) tensor(713.1043)\n",
      "tensor(363.4802) tensor(520.8619) tensor(620.4001) tensor(716.3097)\n",
      "tensor(622.4288) tensor(548.6774) tensor(825.7618) tensor(697.4603)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 bottle, 130.7ms\n",
      "Speed: 2.0ms preprocess, 130.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.3829) tensor(545.7865) tensor(66.9346) tensor(618.6420)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 cell phone, 136.6ms\n",
      "Speed: 2.0ms preprocess, 136.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(447.6937) tensor(537.2880) tensor(552.9523) tensor(623.7526)\n",
      "tensor(873.4607) tensor(252.9283) tensor(1278.9167) tensor(713.9742)\n",
      "tensor(33.5943) tensor(544.4856) tensor(68.5571) tensor(615.5564)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 2 tvs, 146.6ms\n",
      "Speed: 3.0ms preprocess, 146.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(879.3247) tensor(253.4660) tensor(1279.2637) tensor(712.4778)\n",
      "tensor(0.7377) tensor(294.6204) tensor(149.2691) tensor(524.9950)\n",
      "tensor(33.6713) tensor(544.6061) tensor(68.7860) tensor(617.4310)\n",
      "tensor(133.1139) tensor(372.4314) tensor(206.7374) tensor(520.4809)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 cup, 2 tvs, 143.6ms\n",
      "Speed: 3.0ms preprocess, 143.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(133.5745) tensor(374.8547) tensor(208.4211) tensor(501.4116)\n",
      "tensor(34.9932) tensor(544.6387) tensor(68.9229) tensor(616.0510)\n",
      "tensor(875.8303) tensor(250.4829) tensor(1279.0955) tensor(713.6172)\n",
      "tensor(70.7572) tensor(559.6707) tensor(111.4615) tensor(605.6093)\n",
      "tensor(579.2052) tensor(36.1354) tensor(1125.4261) tensor(606.3828)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 (no detections), 135.6ms\n",
      "Speed: 3.0ms preprocess, 135.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.6ms\n",
      "Speed: 3.0ms preprocess, 137.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(875.8702) tensor(254.8563) tensor(1279.0542) tensor(712.5324)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 (no detections), 150.6ms\n",
      "Speed: 3.0ms preprocess, 150.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 (no detections), 141.6ms\n",
      "Speed: 3.0ms preprocess, 141.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 135.6ms\n",
      "Speed: 4.0ms preprocess, 135.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(874.1548) tensor(253.9875) tensor(1279.0186) tensor(710.6624)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 138.6ms\n",
      "Speed: 4.0ms preprocess, 138.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(879.1317) tensor(253.7361) tensor(1278.9203) tensor(711.0582)\n",
      "tensor(32.6480) tensor(545.5632) tensor(68.2514) tensor(616.1121)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 tv, 138.6ms\n",
      "Speed: 2.0ms preprocess, 138.6ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(570.8518) tensor(57.7917) tensor(1123.4531) tensor(670.2936)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 tv, 147.6ms\n",
      "Speed: 3.0ms preprocess, 147.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(877.0362) tensor(248.9236) tensor(1279.1431) tensor(712.7726)\n",
      "tensor(577.7850) tensor(59.0424) tensor(1118.1040) tensor(603.0685)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 bottle, 1 tv, 135.6ms\n",
      "Speed: 3.0ms preprocess, 135.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 136.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.9579) tensor(545.0865) tensor(67.8329) tensor(616.6322)\n",
      "tensor(575.8640) tensor(53.9160) tensor(1115.9828) tensor(616.1219)\n",
      "tensor(30.7764) tensor(546.8746) tensor(68.3294) tensor(618.0422)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 136.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 137.6ms\n",
      "Speed: 3.0ms preprocess, 137.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33.7110) tensor(545.0945) tensor(67.7180) tensor(617.5795)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 bottle, 141.6ms\n",
      "Speed: 3.0ms preprocess, 141.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.6098) tensor(547.3553) tensor(67.9659) tensor(619.6661)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 (no detections), 153.6ms\n",
      "Speed: 3.0ms preprocess, 153.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 136.6ms\n",
      "Speed: 3.0ms preprocess, 136.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.7602) tensor(547.6760) tensor(67.9039) tensor(616.9402)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 (no detections), 138.6ms\n",
      "Speed: 3.0ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 143.6ms\n",
      "Speed: 3.0ms preprocess, 143.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(880.7025) tensor(126.0175) tensor(1280.) tensor(713.8933)\n",
      "tensor(32.1412) tensor(546.1814) tensor(68.6028) tensor(618.0066)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 bottle, 136.6ms\n",
      "Speed: 4.0ms preprocess, 136.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(596.6223) tensor(81.9440) tensor(1280.) tensor(715.5345)\n",
      "tensor(303.7735) tensor(97.3268) tensor(1278.3516) tensor(713.2686)\n",
      "tensor(830.2823) tensor(80.0034) tensor(1279.8043) tensor(719.9585)\n",
      "tensor(35.1812) tensor(545.5615) tensor(69.7482) tensor(614.9480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 chair, 134.6ms\n",
      "Speed: 3.0ms preprocess, 134.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(444.1779) tensor(342.0443) tensor(868.4435) tensor(710.7892)\n",
      "tensor(365.0035) tensor(615.9839) tensor(461.7660) tensor(720.)\n",
      "tensor(31.0138) tensor(545.7822) tensor(67.5352) tensor(618.7864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 127.7ms\n",
      "Speed: 2.0ms preprocess, 127.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(463.1816) tensor(22.7736) tensor(1203.8729) tensor(709.5554)\n",
      "tensor(846.8580) tensor(27.1534) tensor(1275.8571) tensor(714.7667)\n",
      "tensor(32.2581) tensor(545.0159) tensor(69.6015) tensor(622.3487)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 cell phone, 137.6ms\n",
      "Speed: 2.0ms preprocess, 137.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(806.6763) tensor(251.0712) tensor(1278.7478) tensor(713.1893)\n",
      "tensor(1016.8215) tensor(544.3634) tensor(1179.4138) tensor(621.6847)\n",
      "tensor(32.0885) tensor(545.2336) tensor(67.2591) tensor(616.2345)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 138.6ms\n",
      "Speed: 3.0ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(829.9667) tensor(254.4995) tensor(1278.4254) tensor(711.3525)\n",
      "tensor(32.4352) tensor(545.9839) tensor(67.1619) tensor(617.4737)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 128.7ms\n",
      "Speed: 3.0ms preprocess, 128.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(899.3885) tensor(249.6375) tensor(1278.5652) tensor(711.3938)\n",
      "tensor(32.9771) tensor(545.3549) tensor(69.1327) tensor(617.9218)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 130.7ms\n",
      "Speed: 2.0ms preprocess, 130.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 122.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1001.8467) tensor(247.6464) tensor(1278.4705) tensor(712.9306)\n",
      "tensor(32.4115) tensor(545.4331) tensor(68.9675) tensor(617.2723)\n",
      "tensor(964.6494) tensor(246.0406) tensor(1278.9060) tensor(711.6838)\n",
      "tensor(33.9065) tensor(545.3943) tensor(68.7363) tensor(617.0072)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 122.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 143.6ms\n",
      "Speed: 3.0ms preprocess, 143.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 cup, 131.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(914.4716) tensor(252.8242) tensor(1278.9550) tensor(709.3088)\n",
      "tensor(872.3041) tensor(254.8146) tensor(1279.0245) tensor(709.5483)\n",
      "tensor(31.9670) tensor(545.8781) tensor(67.4522) tensor(616.0367)\n",
      "tensor(71.1334) tensor(559.9493) tensor(111.8690) tensor(614.7672)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 131.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 127.7ms\n",
      "Speed: 2.0ms preprocess, 127.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 135.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(836.2361) tensor(249.3360) tensor(1278.9539) tensor(710.0175)\n",
      "tensor(31.7909) tensor(546.0107) tensor(67.5174) tensor(618.6738)\n",
      "tensor(813.4330) tensor(252.0695) tensor(1279.1411) tensor(709.8301)\n",
      "tensor(32.2720) tensor(545.0424) tensor(68.9180) tensor(617.1447)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 135.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 129.7ms\n",
      "Speed: 3.0ms preprocess, 129.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(804.6600) tensor(248.5413) tensor(1278.2374) tensor(709.5856)\n",
      "tensor(136.3555) tensor(283.2128) tensor(223.0168) tensor(374.4339)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 146.6ms\n",
      "Speed: 3.0ms preprocess, 146.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(795.7926) tensor(253.5436) tensor(1279.2584) tensor(710.4941)\n",
      "tensor(31.0276) tensor(545.4348) tensor(68.5249) tensor(616.4745)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 128.7ms\n",
      "Speed: 3.0ms preprocess, 128.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 124.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(785.0910) tensor(247.1532) tensor(1278.4075) tensor(711.1586)\n",
      "tensor(31.0309) tensor(544.9991) tensor(67.8627) tensor(615.8934)\n",
      "tensor(773.4920) tensor(248.9658) tensor(1278.1184) tensor(709.8528)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 124.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 132.6ms\n",
      "Speed: 3.0ms preprocess, 132.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 cup, 127.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(757.5037) tensor(248.9862) tensor(1277.8309) tensor(710.2914)\n",
      "tensor(775.4976) tensor(245.0869) tensor(1278.4854) tensor(710.0975)\n",
      "tensor(690.4026) tensor(591.8931) tensor(747.8835) tensor(668.5251)\n",
      "tensor(33.1831) tensor(545.7689) tensor(68.5465) tensor(614.9999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 127.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 122.7ms\n",
      "Speed: 3.0ms preprocess, 122.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(963.6110) tensor(251.7333) tensor(1279.2819) tensor(710.9003)\n",
      "tensor(33.3501) tensor(546.6907) tensor(67.7622) tensor(615.4346)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 141.6ms\n",
      "Speed: 3.0ms preprocess, 141.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(774.8501) tensor(247.1038) tensor(1278.2703) tensor(710.7871)\n",
      "tensor(30.8749) tensor(547.0874) tensor(68.5784) tensor(617.0232)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 154.6ms\n",
      "Speed: 2.0ms preprocess, 154.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 (no detections), 135.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(763.1740) tensor(249.1573) tensor(1278.3147) tensor(709.9443)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 135.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 (no detections), 135.6ms\n",
      "Speed: 3.0ms preprocess, 135.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 (no detections), 141.6ms\n",
      "Speed: 2.0ms preprocess, 141.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 128.7ms\n",
      "Speed: 2.0ms preprocess, 128.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 129.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(142.5452) tensor(277.7969) tensor(223.4511) tensor(368.0664)\n",
      "tensor(290.7790) tensor(250.6411) tensor(1277.9275) tensor(709.0632)\n",
      "tensor(33.5877) tensor(546.0161) tensor(68.1338) tensor(615.3764)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 129.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.6ms\n",
      "Speed: 2.0ms preprocess, 140.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(960.4366) tensor(251.6822) tensor(1279.3336) tensor(710.6379)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 147.6ms\n",
      "Speed: 3.0ms preprocess, 147.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(34.1490) tensor(545.6903) tensor(68.3082) tensor(616.6432)\n",
      "tensor(975.0015) tensor(251.3486) tensor(1279.2288) tensor(711.8622)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 128.7ms\n",
      "Speed: 3.0ms preprocess, 128.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 donut, 1 refrigerator, 130.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(966.3304) tensor(250.4354) tensor(1279.3739) tensor(711.3820)\n",
      "tensor(477.7187) tensor(499.8132) tensor(668.5927) tensor(649.6426)\n",
      "tensor(256.7178) tensor(151.0538) tensor(780.0781) tensor(716.2632)\n",
      "tensor(767.0858) tensor(246.2393) tensor(1278.2170) tensor(711.0097)\n",
      "tensor(987.0310) tensor(251.3433) tensor(1279.1958) tensor(711.6035)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 130.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 tv, 134.6ms\n",
      "Speed: 1.0ms preprocess, 134.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.7288) tensor(545.2830) tensor(67.6389) tensor(616.6267)\n",
      "tensor(770.8945) tensor(248.6463) tensor(1278.0562) tensor(710.6640)\n",
      "tensor(119.6719) tensor(273.8636) tensor(218.2754) tensor(406.3747)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 118.7ms\n",
      "Speed: 2.0ms preprocess, 118.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(960.5295) tensor(252.0689) tensor(1279.3625) tensor(711.9881)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 (no detections), 135.6ms\n",
      "Speed: 2.0ms preprocess, 135.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 potted plant, 133.3ms\n",
      "Speed: 3.0ms preprocess, 133.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(785.8589) tensor(251.5287) tensor(1279.0703) tensor(709.7262)\n",
      "tensor(31.7296) tensor(545.2681) tensor(68.8194) tensor(616.1984)\n",
      "tensor(82.3503) tensor(347.3568) tensor(309.9205) tensor(709.6558)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 140.6ms\n",
      "Speed: 3.0ms preprocess, 140.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(840.3752) tensor(254.1080) tensor(1279.0652) tensor(709.7904)\n",
      "tensor(120.1815) tensor(339.5320) tensor(386.5129) tensor(713.4207)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 cat, 1 bottle, 136.6ms\n",
      "Speed: 2.0ms preprocess, 136.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(886.7348) tensor(249.4667) tensor(1278.8066) tensor(710.5489)\n",
      "tensor(199.1380) tensor(327.7854) tensor(476.7951) tensor(595.6569)\n",
      "tensor(32.9699) tensor(545.5629) tensor(67.7140) tensor(619.5396)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 135.6ms\n",
      "Speed: 2.0ms preprocess, 135.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(930.0818) tensor(251.5917) tensor(1278.9690) tensor(710.6392)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 121.7ms\n",
      "Speed: 3.0ms preprocess, 121.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(950.3314) tensor(250.1367) tensor(1278.7385) tensor(711.2979)\n",
      "tensor(322.7593) tensor(329.6072) tensor(598.7461) tensor(712.4996)\n",
      "tensor(30.6995) tensor(545.5739) tensor(65.9756) tensor(616.8304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 134.6ms\n",
      "Speed: 3.0ms preprocess, 134.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(955.2280) tensor(248.6758) tensor(1279.0059) tensor(709.8766)\n",
      "tensor(345.1580) tensor(331.4442) tensor(608.7775) tensor(714.9303)\n",
      "tensor(32.5999) tensor(544.6663) tensor(68.3291) tensor(615.3779)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 132.6ms\n",
      "Speed: 2.0ms preprocess, 132.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(353.3915) tensor(329.1140) tensor(588.6848) tensor(713.9182)\n",
      "tensor(996.8214) tensor(249.7383) tensor(1279.0840) tensor(710.2282)\n",
      "tensor(31.2320) tensor(546.1128) tensor(67.7515) tensor(618.0253)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 132.6ms\n",
      "Speed: 3.0ms preprocess, 132.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(961.3781) tensor(247.9834) tensor(1279.0883) tensor(710.1144)\n",
      "tensor(365.0103) tensor(329.9857) tensor(653.8145) tensor(710.4462)\n",
      "tensor(429.7673) tensor(448.5981) tensor(520.5229) tensor(618.5485)\n",
      "tensor(30.4900) tensor(545.2939) tensor(70.0305) tensor(615.8516)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 139.6ms\n",
      "Speed: 3.0ms preprocess, 139.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(367.2957) tensor(333.7203) tensor(653.9641) tensor(713.2223)\n",
      "tensor(940.9287) tensor(251.2825) tensor(1279.0054) tensor(710.6117)\n",
      "tensor(32.5028) tensor(546.6736) tensor(67.2748) tensor(618.0205)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 134.6ms\n",
      "Speed: 3.0ms preprocess, 134.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(370.0509) tensor(332.6814) tensor(680.2330) tensor(713.7413)\n",
      "tensor(963.5958) tensor(248.7546) tensor(1279.0006) tensor(710.0907)\n",
      "tensor(31.2436) tensor(545.5157) tensor(67.6297) tensor(617.8235)\n",
      "tensor(432.7256) tensor(449.3805) tensor(539.8672) tensor(606.1061)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 2 cell phones, 131.6ms\n",
      "Speed: 2.0ms preprocess, 131.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(911.7957) tensor(251.1439) tensor(1278.9387) tensor(712.5760)\n",
      "tensor(363.3176) tensor(336.1562) tensor(697.2905) tensor(712.7014)\n",
      "tensor(420.5881) tensor(443.0637) tensor(522.4812) tensor(615.5538)\n",
      "tensor(452.6710) tensor(499.6232) tensor(522.0679) tensor(610.9315)\n",
      "tensor(33.4961) tensor(545.7362) tensor(66.9845) tensor(616.4852)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 133.6ms\n",
      "Speed: 3.0ms preprocess, 133.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(965.4567) tensor(251.6970) tensor(1279.1218) tensor(711.0535)\n",
      "tensor(351.6491) tensor(339.5485) tensor(753.3626) tensor(712.5909)\n",
      "tensor(31.6223) tensor(547.3099) tensor(67.1324) tensor(618.6873)\n",
      "tensor(415.7081) tensor(453.7960) tensor(519.4847) tensor(627.2188)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 135.6ms\n",
      "Speed: 3.0ms preprocess, 135.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(361.3310) tensor(338.2060) tensor(923.2063) tensor(712.9748)\n",
      "tensor(855.1858) tensor(248.8855) tensor(1278.9058) tensor(715.0544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cup, 1 bed, 141.6ms\n",
      "Speed: 3.0ms preprocess, 141.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(364.8835) tensor(334.2856) tensor(905.6501) tensor(712.5972)\n",
      "tensor(31.7385) tensor(545.6451) tensor(67.2377) tensor(618.9491)\n",
      "tensor(859.9608) tensor(248.8828) tensor(1278.7931) tensor(712.3518)\n",
      "tensor(71.7316) tensor(559.9768) tensor(110.0380) tensor(613.0543)\n",
      "tensor(856.1719) tensor(246.7217) tensor(1278.9893) tensor(713.7385)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 bed, 1 cell phone, 142.6ms\n",
      "Speed: 2.0ms preprocess, 142.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(363.2109) tensor(328.3529) tensor(910.5308) tensor(712.2321)\n",
      "tensor(856.1544) tensor(249.9510) tensor(1278.7417) tensor(711.2328)\n",
      "tensor(423.2945) tensor(453.0947) tensor(522.5319) tensor(621.9683)\n",
      "tensor(854.9091) tensor(243.6948) tensor(1278.5161) tensor(713.4103)\n",
      "tensor(32.1975) tensor(545.6359) tensor(68.2744) tensor(616.5962)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 130.6ms\n",
      "Speed: 3.0ms preprocess, 130.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(330.7999) tensor(317.2883) tensor(896.8671) tensor(713.5051)\n",
      "tensor(857.2404) tensor(251.7240) tensor(1278.6649) tensor(711.8036)\n",
      "tensor(521.8089) tensor(498.7985) tensor(650.3088) tensor(672.2127)\n",
      "tensor(342.8586) tensor(300.9969) tensor(888.8473) tensor(713.7321)\n",
      "tensor(857.6385) tensor(242.0966) tensor(1278.6404) tensor(712.5223)\n",
      "tensor(855.1461) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bed, 120.7ms\n",
      "Speed: 2.0ms preprocess, 120.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(342.0984) tensor(1279.0072) tensor(712.9622)\n",
      "tensor(4.4595) tensor(305.7504) tensor(866.4413) tensor(715.3325)\n",
      "tensor(854.6196) tensor(255.1881) tensor(1278.7622) tensor(713.0403)\n",
      "tensor(291.2163) tensor(303.1001) tensor(863.1803) tensor(711.9756)\n",
      "tensor(539.8894) tensor(456.5895) tensor(579.4631) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 tv, 1 cell phone, 133.6ms\n",
      "Speed: 3.0ms preprocess, 133.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(545.1785)\n",
      "tensor(0.3041) tensor(156.6913) tensor(239.2834) tensor(593.6371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 bed, 1 cell phone, 146.6ms\n",
      "Speed: 2.0ms preprocess, 146.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(77.1818) tensor(294.8040) tensor(856.3083) tensor(712.9417)\n",
      "tensor(457.3235) tensor(468.9972) tensor(530.9930) tensor(568.2683)\n",
      "tensor(855.8390) tensor(247.2890) tensor(1278.6198) tensor(714.6290)\n",
      "tensor(854.5159) tensor(245.4688) tensor(1279.2214) tensor(714.7074)\n",
      "tensor(32.2208) tensor(547.0691) tensor(67.5669) tensor(621.8228)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 137.6ms\n",
      "Speed: 3.0ms preprocess, 137.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(285.3358) tensor(884.7945) tensor(714.7103)\n",
      "tensor(858.2032) tensor(251.5906) tensor(1278.5249) tensor(709.0898)\n",
      "tensor(339.4170) tensor(285.6103) tensor(868.3269) tensor(714.5347)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 131.6ms\n",
      "Speed: 3.0ms preprocess, 131.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(290.5200) tensor(869.8488) tensor(714.3639)\n",
      "tensor(820.7245) tensor(251.0698) tensor(1278.9191) tensor(712.5926)\n",
      "tensor(372.0332) tensor(289.0612) tensor(962.2656) tensor(713.9829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 138.6ms\n",
      "Speed: 2.0ms preprocess, 138.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(583.9360) tensor(299.5414) tensor(898.9460) tensor(716.0195)\n",
      "tensor(285.3867) tensor(302.0931) tensor(896.1732) tensor(716.0651)\n",
      "tensor(1012.9495) tensor(246.8065) tensor(1279.0710) tensor(710.2456)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 151.6ms\n",
      "Speed: 3.0ms preprocess, 151.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(542.9531) tensor(301.2598) tensor(902.2809) tensor(714.6688)\n",
      "tensor(887.4897) tensor(249.7791) tensor(1278.8596) tensor(712.4338)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 bottle, 1 cup, 1 refrigerator, 148.6ms\n",
      "Speed: 3.0ms preprocess, 148.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(567.3112) tensor(305.3792) tensor(920.7318) tensor(718.6943)\n",
      "tensor(1001.8402) tensor(249.9899) tensor(1279.0968) tensor(711.1126)\n",
      "tensor(314.2231) tensor(305.7203) tensor(922.9806) tensor(715.5775)\n",
      "tensor(310.5619) tensor(181.0843) tensor(763.7358) tensor(715.9531)\n",
      "tensor(71.3608) tensor(560.8276) tensor(109.7698) tensor(613.9883)\n",
      "tensor(31.9470) tensor(547.0221) tensor(67.0622) tensor(617.7972)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 143.6ms\n",
      "Speed: 2.0ms preprocess, 143.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(898.6807) tensor(252.4640) tensor(1279.1270) tensor(710.2957)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 cup, 144.6ms\n",
      "Speed: 3.0ms preprocess, 144.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(882.0161) tensor(252.9990) tensor(1279.0955) tensor(710.1184)\n",
      "tensor(32.3239) tensor(546.8102) tensor(67.1073) tensor(617.6231)\n",
      "tensor(70.8931) tensor(560.5079) tensor(110.2166) tensor(611.9379)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 148.6ms\n",
      "Speed: 3.0ms preprocess, 148.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(880.4536) tensor(249.2372) tensor(1279.0806) tensor(711.6591)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 145.6ms\n",
      "Speed: 3.0ms preprocess, 145.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(887.6772) tensor(250.9138) tensor(1279.1086) tensor(711.7905)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 donut, 139.6ms\n",
      "Speed: 3.0ms preprocess, 139.6ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(886.2767) tensor(248.3923) tensor(1279.0853) tensor(711.9519)\n",
      "tensor(559.5618) tensor(534.8217) tensor(634.2181) tensor(578.6943)\n",
      "tensor(31.5596) tensor(547.1841) tensor(68.3024) tensor(619.8982)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 donut, 142.6ms\n",
      "Speed: 3.0ms preprocess, 142.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(561.5090) tensor(538.5728) tensor(639.0872) tensor(582.1114)\n",
      "tensor(912.6743) tensor(248.6674) tensor(1278.5608) tensor(712.4611)\n",
      "tensor(33.6147) tensor(546.5990) tensor(67.5541) tensor(623.6241)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 donut, 1 refrigerator, 146.6ms\n",
      "Speed: 3.0ms preprocess, 146.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(909.5713) tensor(251.5525) tensor(1279.1379) tensor(710.9064)\n",
      "tensor(567.4700) tensor(551.0295) tensor(644.7524) tensor(604.0540)\n",
      "tensor(463.7286) tensor(294.1024) tensor(926.7014) tensor(716.0026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 donut, 134.6ms\n",
      "Speed: 3.0ms preprocess, 134.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(906.2755) tensor(249.9857) tensor(1279.0721) tensor(711.2938)\n",
      "tensor(573.4114) tensor(552.7504) tensor(652.0759) tensor(607.6136)\n",
      "tensor(303.6455) tensor(305.5853) tensor(513.8532) tensor(713.3002)\n",
      "tensor(33.2171) tensor(546.2242) tensor(67.2614) tensor(618.1827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 refrigerator, 163.6ms\n",
      "Speed: 3.0ms preprocess, 163.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(911.2019) tensor(252.2244) tensor(1279.0251) tensor(710.0602)\n",
      "tensor(485.1772) tensor(300.7863) tensor(948.5489) tensor(715.2414)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 refrigerator, 152.6ms\n",
      "Speed: 3.0ms preprocess, 152.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(257.2330) tensor(300.3720) tensor(554.4742) tensor(711.6713)\n",
      "tensor(951.8917) tensor(248.2143) tensor(1278.9844) tensor(710.7683)\n",
      "tensor(514.1696) tensor(291.0469) tensor(964.7958) tensor(714.5518)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 donut, 142.6ms\n",
      "Speed: 3.0ms preprocess, 142.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(264.7408) tensor(299.0358) tensor(565.9955) tensor(711.8430)\n",
      "tensor(949.6108) tensor(249.8937) tensor(1279.1707) tensor(710.9531)\n",
      "tensor(625.1847) tensor(521.5958) tensor(712.7335) tensor(575.9183)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 142.6ms\n",
      "Speed: 3.0ms preprocess, 142.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(275.2889) tensor(297.6038) tensor(577.6414) tensor(712.6680)\n",
      "tensor(958.3490) tensor(250.3480) tensor(1279.2179) tensor(710.4221)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 donut, 1 cell phone, 147.6ms\n",
      "Speed: 3.0ms preprocess, 147.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 refrigerator, 136.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(276.8644) tensor(295.9086) tensor(609.0647) tensor(711.3162)\n",
      "tensor(957.8473) tensor(250.6521) tensor(1279.1322) tensor(710.7812)\n",
      "tensor(450.6024) tensor(425.4755) tensor(530.3510) tensor(493.8781)\n",
      "tensor(638.4546) tensor(503.4556) tensor(726.8210) tensor(549.3503)\n",
      "tensor(285.4691) tensor(293.3048) tensor(591.8868) tensor(712.6876)\n",
      "tensor(961.2446) tensor(246.6558) tensor(1279.0264) tensor(711.8499)\n",
      "tensor(0.6039) tensor(261.7317) tensor(219.8844) tensor(713.5328)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 136.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 137.6ms\n",
      "Speed: 3.0ms preprocess, 137.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(292.9183) tensor(296.3072) tensor(631.6451) tensor(713.7021)\n",
      "tensor(421.5544) tensor(425.9142) tensor(504.5393) tensor(488.8242)\n",
      "tensor(961.6254) tensor(250.3057) tensor(1279.0631) tensor(711.7233)\n",
      "tensor(33.3828) tensor(546.4555) tensor(68.0458) tensor(618.1190)\n",
      "tensor(295.7313) tensor(299.4794) tensor(590.3726) tensor(713.0915)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 126.7ms\n",
      "Speed: 3.0ms preprocess, 126.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(958.5649) tensor(248.5217) tensor(1279.1177) tensor(711.4313)\n",
      "tensor(33.5535) tensor(546.4075) tensor(68.0547) tensor(619.9481)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 145.6ms\n",
      "Speed: 4.0ms preprocess, 145.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(293.8367) tensor(296.5706) tensor(644.9339) tensor(713.5452)\n",
      "tensor(962.1617) tensor(247.7990) tensor(1279.1567) tensor(710.9884)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 2 cell phones, 141.6ms\n",
      "Speed: 2.0ms preprocess, 141.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(291.3099) tensor(298.4984) tensor(588.2052) tensor(715.3276)\n",
      "tensor(398.7237) tensor(424.1990) tensor(493.2835) tensor(481.9441)\n",
      "tensor(522.9642) tensor(496.6872) tensor(585.2023) tensor(556.6266)\n",
      "tensor(964.6937) tensor(249.6346) tensor(1279.1532) tensor(710.5251)\n",
      "tensor(31.4043) tensor(546.5676) tensor(67.6862) tensor(616.6215)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 1 refrigerator, 138.6ms\n",
      "Speed: 3.0ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(289.7369) tensor(298.7766) tensor(666.8823) tensor(714.5177)\n",
      "tensor(983.7114) tensor(249.3835) tensor(1279.1907) tensor(711.2809)\n",
      "tensor(404.1016) tensor(421.7719) tensor(478.4703) tensor(505.3065)\n",
      "tensor(571.6244) tensor(178.4159) tensor(1028.5122) tensor(713.7203)\n",
      "tensor(32.0113) tensor(547.3784) tensor(66.6356) tensor(619.1414)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 2 cell phones, 123.7ms\n",
      "Speed: 2.0ms preprocess, 123.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(299.2487) tensor(292.8946) tensor(645.4845) tensor(713.8743)\n",
      "tensor(1021.1405) tensor(251.6318) tensor(1279.1879) tensor(712.2150)\n",
      "tensor(436.1052) tensor(392.1030) tensor(524.7341) tensor(476.3400)\n",
      "tensor(624.0319) tensor(595.1157) tensor(713.9650) tensor(703.2971)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 2 donuts, 1 tv, 1 cell phone, 126.7ms\n",
      "Speed: 2.0ms preprocess, 126.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(305.4297) tensor(294.7195) tensor(647.8237) tensor(713.6150)\n",
      "tensor(940.7888) tensor(253.6838) tensor(1278.9797) tensor(712.4702)\n",
      "tensor(438.8284) tensor(403.4370) tensor(524.4038) tensor(479.9069)\n",
      "tensor(728.3157) tensor(383.9014) tensor(838.5742) tensor(443.1314)\n",
      "tensor(1.0314) tensor(241.9614) tensor(221.8664) tensor(687.2429)\n",
      "tensor(732.8594) tensor(518.2681) tensor(774.1716) tensor(561.2567)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 2 donuts, 2 cell phones, 1 refrigerator, 127.7ms\n",
      "Speed: 2.0ms preprocess, 127.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(304.5820) tensor(295.8731) tensor(648.0005) tensor(714.4650)\n",
      "tensor(0.8224) tensor(290.2870) tensor(218.4725) tensor(713.7317)\n",
      "tensor(940.5260) tensor(252.4681) tensor(1278.9437) tensor(711.4628)\n",
      "tensor(445.3807) tensor(400.9931) tensor(537.8714) tensor(476.0495)\n",
      "tensor(733.2006) tensor(378.7614) tensor(848.1514) tensor(446.2623)\n",
      "tensor(734.0581) tensor(379.6555) tensor(845.8665) tensor(426.9319)\n",
      "tensor(445.6927) tensor(400.8669) tensor(537.0865) tensor(453.4064)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 1 refrigerator, 134.6ms\n",
      "Speed: 2.0ms preprocess, 134.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(307.0892) tensor(294.8302) tensor(654.8094) tensor(713.5150)\n",
      "tensor(462.6828) tensor(387.0140) tensor(555.1270) tensor(449.1365)\n",
      "tensor(31.4760) tensor(546.4263) tensor(67.1629) tensor(618.9250)\n",
      "tensor(949.3965) tensor(251.5792) tensor(1279.2354) tensor(709.0406)\n",
      "tensor(630.9438) tensor(91.9825) tensor(1087.1976) tensor(697.8707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 126.7ms\n",
      "Speed: 2.0ms preprocess, 126.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(305.9547) tensor(295.2851) tensor(649.7684) tensor(711.3405)\n",
      "tensor(943.4656) tensor(255.0146) tensor(1279.0049) tensor(710.8089)\n",
      "tensor(467.3551) tensor(473.4569) tensor(609.2274) tensor(663.0546)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 donut, 136.6ms\n",
      "Speed: 3.0ms preprocess, 136.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(307.2206) tensor(295.9039) tensor(649.0387) tensor(714.2391)\n",
      "tensor(734.1926) tensor(319.5457) tensor(851.3167) tensor(369.7138)\n",
      "tensor(952.1345) tensor(247.7297) tensor(1279.2456) tensor(709.8696)\n",
      "tensor(32.3459) tensor(547.9138) tensor(67.2868) tensor(618.5372)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 donut, 139.6ms\n",
      "Speed: 3.0ms preprocess, 139.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(312.1935) tensor(294.6537) tensor(666.9189) tensor(713.4928)\n",
      "tensor(941.8578) tensor(248.7407) tensor(1279.2397) tensor(709.3541)\n",
      "tensor(743.6270) tensor(318.3358) tensor(856.0718) tensor(365.4958)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 2 cell phones, 127.7ms\n",
      "Speed: 3.0ms preprocess, 127.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(465.5263) tensor(484.9376) tensor(633.5093) tensor(636.8668)\n",
      "tensor(1002.1995) tensor(249.4435) tensor(1279.4553) tensor(710.1221)\n",
      "tensor(326.4136) tensor(295.2199) tensor(773.9561) tensor(712.4377)\n",
      "tensor(533.3358) tensor(429.2141) tensor(570.2760) tensor(487.9167)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 138.6ms\n",
      "Speed: 3.0ms preprocess, 138.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 132.6ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(341.9485) tensor(302.3598) tensor(800.7056) tensor(713.7852)\n",
      "tensor(499.4365) tensor(477.9673) tensor(618.0742) tensor(628.0573)\n",
      "tensor(905.8320) tensor(251.2219) tensor(1278.8911) tensor(711.2938)\n",
      "tensor(31.8953) tensor(547.2275) tensor(68.9765) tensor(619.8340)\n",
      "tensor(332.9204) tensor(304.6493) tensor(808.4541) tensor(714.0719)\n",
      "tensor(961.9667) tensor(250.9737) tensor(1279.1945) tensor(711.1705)\n",
      "tensor(32.9815) tensor(546.8501) tensor(66.7967) tensor(621.2732)\n",
      "tensor(474.4958) tensor(473.6284) tensor(607.1660) tensor(619.2356)\n",
      "tensor(98.6158) tensor(259.7648) tensor(176.6802) tensor(330.3037)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 132.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 134.6ms\n",
      "Speed: 3.0ms preprocess, 134.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(301.1040) tensor(309.2145) tensor(782.5820) tensor(714.6478)\n",
      "tensor(444.7288) tensor(477.4282) tensor(530.7618) tensor(600.8265)\n",
      "tensor(897.6584) tensor(252.3181) tensor(1279.0659) tensor(710.8483)\n",
      "tensor(32.0364) tensor(546.9368) tensor(66.7708) tensor(621.7896)\n",
      "tensor(288.8602) tensor(317.6841) tensor(714.6629) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 bed, 127.7ms\n",
      "Speed: 3.0ms preprocess, 127.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(713.3174)\n",
      "tensor(857.6060) tensor(250.5496) tensor(1278.8621) tensor(712.9427)\n",
      "tensor(32.8605) tensor(546.6805) tensor(66.0224) tensor(621.7120)\n",
      "tensor(854.0532) tensor(243.4166) tensor(1278.9619) tensor(714.6450)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 140.6ms\n",
      "Speed: 2.0ms preprocess, 140.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(280.2416) tensor(319.1692) tensor(726.9409) tensor(717.1654)\n",
      "tensor(657.4744) tensor(485.9953) tensor(876.2957) tensor(716.4172)\n",
      "tensor(938.3347) tensor(249.9691) tensor(1278.6038) tensor(713.0365)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 127.7ms\n",
      "Speed: 3.0ms preprocess, 127.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 124.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(282.7977) tensor(316.6609) tensor(755.7294) tensor(715.6654)\n",
      "tensor(855.6837) tensor(251.9984) tensor(1278.4955) tensor(710.7787)\n",
      "tensor(33.1599) tensor(546.7498) tensor(66.5094) tensor(617.8612)\n",
      "tensor(293.2930) tensor(321.3629) tensor(832.3882) tensor(713.6108)\n",
      "tensor(871.4635) tensor(249.6530) tensor(1278.4376) tensor(712.6326)\n",
      "tensor(32.1960) tensor(547.1907) tensor(66.0158) tensor(621.0031)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 124.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 tv, 1 laptop, 130.6ms\n",
      "Speed: 3.0ms preprocess, 130.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(288.0356) tensor(332.3810) tensor(744.6503) tensor(713.5302)\n",
      "tensor(853.7485) tensor(251.1564) tensor(1278.4854) tensor(711.6437)\n",
      "tensor(32.6138) tensor(546.5139) tensor(67.6120) tensor(620.4049)\n",
      "tensor(397.8801) tensor(495.4816) tensor(568.1780) tensor(622.9825)\n",
      "tensor(399.9905) tensor(495.2897) tensor(561.4020) tensor(623.0414)\n",
      "tensor(849.7678) tensor(250.6536) tensor(1278.8591) tensor(711.6032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 bed, 122.7ms\n",
      "Speed: 2.0ms preprocess, 122.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(269.9199) tensor(345.5825) tensor(781.4331) tensor(713.5024)\n",
      "tensor(31.9159) tensor(546.7015) tensor(66.7337) tensor(622.1760)\n",
      "tensor(850.5400) tensor(242.7465) tensor(1278.9448) tensor(713.3571)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 refrigerator, 133.6ms\n",
      "Speed: 2.0ms preprocess, 133.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(265.1247) tensor(362.7688) tensor(784.6341) tensor(713.6987)\n",
      "tensor(867.0741) tensor(249.2151) tensor(1278.8915) tensor(710.9508)\n",
      "tensor(32.9815) tensor(546.4271) tensor(67.1847) tensor(622.6231)\n",
      "tensor(442.3882) tensor(146.9092) tensor(778.3179) tensor(596.8212)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 1 refrigerator, 132.6ms\n",
      "Speed: 2.0ms preprocess, 132.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 refrigerator, 128.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(277.4927) tensor(383.1008) tensor(801.7847) tensor(714.7277)\n",
      "tensor(862.4745) tensor(251.1375) tensor(1278.7578) tensor(710.7893)\n",
      "tensor(334.0323) tensor(527.9482) tensor(436.0462) tensor(673.1122)\n",
      "tensor(442.9687) tensor(148.7480) tensor(783.3677) tensor(600.0403)\n",
      "tensor(32.1561) tensor(545.9379) tensor(67.4284) tensor(623.1211)\n",
      "tensor(253.2270) tensor(400.3802) tensor(792.6974) tensor(714.8268)\n",
      "tensor(850.1097) tensor(251.7016) tensor(1278.6965) tensor(710.9843)\n",
      "tensor(31.0952) tensor(546.4200) tensor(66.8700) tensor(621.0021)\n",
      "tensor(441.6005) tensor(149.6953) tensor(780.0464) tensor(603.9952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 128.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 126.7ms\n",
      "Speed: 3.0ms preprocess, 126.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(851.1471) tensor(247.9796) tensor(1278.7545) tensor(711.3790)\n",
      "tensor(239.3849) tensor(403.3682) tensor(745.9178) tensor(713.3986)\n",
      "tensor(32.0572) tensor(545.9153) tensor(68.1803) tensor(622.7487)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 2 cell phones, 138.6ms\n",
      "Speed: 2.0ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 117.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(278.6360) tensor(390.6148) tensor(775.0059) tensor(715.3385)\n",
      "tensor(366.5064) tensor(493.4836) tensor(453.3979) tensor(673.2173)\n",
      "tensor(847.8644) tensor(251.0499) tensor(1278.6210) tensor(711.5209)\n",
      "tensor(371.2990) tensor(494.0993) tensor(454.5863) tensor(573.9128)\n",
      "tensor(32.6636) tensor(546.4601) tensor(65.0291) tensor(617.4240)\n",
      "tensor(286.1148) tensor(375.3558) tensor(842.9905) tensor(714.1176)\n",
      "tensor(852.6945) tensor(252.5657) tensor(1278.8806) tensor(711.4501)\n",
      "tensor(30.3720) tensor(546.6875) tensor(66.6438) tensor(619.3918)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 117.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 cell phone, 141.6ms\n",
      "Speed: 3.0ms preprocess, 141.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(332.0815) tensor(363.1960) tensor(895.3070) tensor(714.0214)\n",
      "tensor(526.5070) tensor(459.1095) tensor(602.1245) tensor(520.5034)\n",
      "tensor(709.0901) tensor(249.5739) tensor(1278.6022) tensor(711.3400)\n",
      "tensor(332.4604) tensor(173.6905) tensor(897.1364) tensor(714.1845)\n",
      "tensor(33.0900) tensor(547.2050) tensor(66.4150) tensor(621.2001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 tv, 147.6ms\n",
      "Speed: 2.0ms preprocess, 147.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(742.5042) tensor(245.3045) tensor(1278.6010) tensor(711.5086)\n",
      "tensor(374.2284) tensor(356.4604) tensor(915.6953) tensor(712.5912)\n",
      "tensor(0.2050) tensor(157.4192) tensor(238.8922) tensor(550.6827)\n",
      "tensor(0.3547) tensor(423.7961) tensor(228.3423) tensor(714.1053)\n",
      "tensor(875.6486) tensor(250.0380) tensor(1278.7961) tensor(711.8388)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 124.7ms\n",
      "Speed: 2.0ms preprocess, 124.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(445.1242) tensor(353.2133) tensor(927.8674) tensor(708.1135)\n",
      "tensor(193.2319) tensor(358.6255) tensor(826.5878) tensor(712.9360)\n",
      "tensor(66.1536) tensor(286.1013) tensor(893.3474) tensor(711.3396)\n",
      "tensor(613.5823) tensor(439.1841) tensor(684.1282) tensor(502.4584)\n",
      "tensor(593.4941) tensor(248.8611) tensor(1278.7217) tensor(711.7698)\n",
      "tensor(232.4229) tensor(344.2408) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 1 cell phone, 128.7ms\n",
      "Speed: 3.0ms preprocess, 128.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(927.7842) tensor(709.8755)\n",
      "tensor(898.8187) tensor(255.8021) tensor(1278.8044) tensor(711.3586)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 135.6ms\n",
      "Speed: 2.0ms preprocess, 135.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(135.5493) tensor(118.5602) tensor(944.5479) tensor(711.9364)\n",
      "tensor(32.3132) tensor(547.1589) tensor(66.0269) tensor(620.1648)\n",
      "tensor(885.9564) tensor(253.5618) tensor(1278.9690) tensor(710.8598)\n",
      "tensor(898.3981) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 2 beds, 136.6ms\n",
      "Speed: 3.0ms preprocess, 136.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(247.6989) tensor(1278.5292) tensor(711.5278)\n",
      "tensor(196.0108) tensor(47.1688) tensor(946.4501) tensor(711.0953)\n",
      "tensor(166.8096) tensor(195.0541) tensor(795.6816) tensor(491.8984)\n",
      "tensor(133.4680) tensor(118.4698) tensor(952.4697) tensor(713.1682)\n",
      "tensor(31.0230) tensor(547.0012) tensor(66.5504) tensor(619.2877)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 cell phone, 354.1ms\n",
      "Speed: 2.0ms preprocess, 354.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bed, 122.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(772.3578) tensor(249.2892) tensor(1278.3164) tensor(712.2817)\n",
      "tensor(170.3177) tensor(333.1847) tensor(220.8465) tensor(388.5811)\n",
      "tensor(30.4649) tensor(547.0490) tensor(66.3156) tensor(620.7545)\n",
      "tensor(772.4094) tensor(250.7837) tensor(1278.1516) tensor(711.0380)\n",
      "tensor(106.3318) tensor(168.5572) tensor(896.3193) tensor(491.8326)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 10.0ms preprocess, 122.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 133.0ms\n",
      "Speed: 3.0ms preprocess, 133.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(894.7659) tensor(255.9400) tensor(1278.9788) tensor(710.7447)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 141.2ms\n",
      "Speed: 3.0ms preprocess, 141.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(909.8917) tensor(253.8264) tensor(1278.9652) tensor(711.1700)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 139.9ms\n",
      "Speed: 2.0ms preprocess, 139.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(771.3214) tensor(246.7433) tensor(1278.2970) tensor(711.7054)\n",
      "tensor(32.1697) tensor(546.9511) tensor(67.4109) tensor(619.8419)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 130.5ms\n",
      "Speed: 2.0ms preprocess, 130.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(905.6673) tensor(250.4967) tensor(1278.9524) tensor(711.5573)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cell phone, 147.1ms\n",
      "Speed: 3.0ms preprocess, 147.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(161.3552) tensor(321.3458) tensor(220.4471) tensor(415.0303)\n",
      "tensor(49.7836) tensor(142.2451) tensor(972.1143) tensor(717.5139)\n",
      "tensor(298.5436) tensor(224.3706) tensor(968.7532) tensor(713.5950)\n",
      "tensor(773.6296) tensor(232.5256) tensor(1278.2668) tensor(712.5211)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 137.9ms\n",
      "Speed: 4.0ms preprocess, 137.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(68.0010) tensor(144.5154) tensor(1040.7812) tensor(717.0906)\n",
      "tensor(768.0765) tensor(238.5129) tensor(1278.2031) tensor(711.5726)\n",
      "tensor(281.2556) tensor(243.4253) tensor(1038.5012) tensor(712.9077)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cat, 138.2ms\n",
      "Speed: 3.0ms preprocess, 138.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(118.8387) tensor(227.2352) tensor(1050.5291) tensor(711.9734)\n",
      "tensor(767.6814) tensor(232.3749) tensor(1277.9773) tensor(711.3353)\n",
      "tensor(228.2233) tensor(125.3900) tensor(883.3560) tensor(461.1519)\n",
      "tensor(284.7786) tensor(228.8999) tensor(1075.8561) tensor(710.8518)\n",
      "tensor(770.4979) tensor(229.7737) tensor(1278.1790) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 138.6ms\n",
      "Speed: 2.0ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 127.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(711.2262)\n",
      "tensor(156.6311) tensor(332.8282) tensor(221.8099) tensor(412.8261)\n",
      "tensor(337.9681) tensor(230.6171) tensor(1179.7279) tensor(712.1548)\n",
      "tensor(760.4662) tensor(234.7482) tensor(1278.4663) tensor(711.1660)\n",
      "tensor(34.0018) tensor(547.0048) tensor(66.8986) tensor(620.5931)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 2.0ms preprocess, 127.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 139.7ms\n",
      "Speed: 3.0ms preprocess, 139.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(372.9787) tensor(229.7216) tensor(1084.2494) tensor(711.3083)\n",
      "tensor(772.4519) tensor(238.0337) tensor(1278.4734) tensor(711.6768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 134.6ms\n",
      "Speed: 3.0ms preprocess, 134.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 135.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(292.1056) tensor(228.5375) tensor(1098.4059) tensor(713.4381)\n",
      "tensor(774.6715) tensor(237.6323) tensor(1278.6393) tensor(711.7130)\n",
      "tensor(770.7362) tensor(229.3383) tensor(1278.5551) tensor(711.8255)\n",
      "tensor(286.1948) tensor(215.9318) tensor(1060.9701) tensor(712.5692)\n",
      "tensor(31.4448) tensor(547.3435) tensor(64.3317) tensor(621.4581)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 135.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 139.6ms\n",
      "Speed: 7.0ms preprocess, 139.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(771.1332) tensor(229.2292) tensor(1278.5284) tensor(712.2929)\n",
      "tensor(152.9431) tensor(221.3545) tensor(1060.4675) tensor(714.4423)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 135.6ms\n",
      "Speed: 2.0ms preprocess, 135.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(772.1943) tensor(234.2867) tensor(1278.3252) tensor(711.9450)\n",
      "tensor(88.8252) tensor(84.0816) tensor(1096.0593) tensor(713.5749)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 4 persons, 1 cell phone, 145.6ms\n",
      "Speed: 4.0ms preprocess, 145.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(315.0120) tensor(237.1104) tensor(1073.2571) tensor(713.0859)\n",
      "tensor(105.9352) tensor(474.9332) tensor(190.0903) tensor(690.1860)\n",
      "tensor(0.2808) tensor(303.4952) tensor(152.3903) tensor(582.4795)\n",
      "tensor(739.9591) tensor(240.8911) tensor(1278.8217) tensor(711.1754)\n",
      "tensor(179.5037) tensor(302.3457) tensor(813.9432) tensor(712.4346)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 132.0ms\n",
      "Speed: 3.0ms preprocess, 132.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(236.3078) tensor(240.7169) tensor(1029.0199) tensor(716.0128)\n",
      "tensor(369.9994) tensor(333.3469) tensor(731.6925) tensor(713.9476)\n",
      "tensor(372.1849) tensor(333.4678) tensor(905.5559) tensor(715.1235)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 bottle, 137.6ms\n",
      "Speed: 3.0ms preprocess, 137.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(354.8627) tensor(330.0912) tensor(927.5517) tensor(712.1949)\n",
      "tensor(317.8357) tensor(362.8973) tensor(814.0160) tensor(713.3961)\n",
      "tensor(32.3937) tensor(546.5722) tensor(65.1799) tensor(620.8809)\n",
      "tensor(890.6216) tensor(247.9333) tensor(1279.0479) tensor(711.7847)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 134.9ms\n",
      "Speed: 2.0ms preprocess, 134.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(839.6517) tensor(247.6469) tensor(1278.5603) tensor(711.8181)\n",
      "tensor(31.1813) tensor(546.7605) tensor(67.0116) tensor(620.6688)\n",
      "tensor(343.7200) tensor(361.3632) tensor(879.6165) tensor(712.7157)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 remote, 135.2ms\n",
      "Speed: 2.0ms preprocess, 135.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(353.2506) tensor(374.5132) tensor(831.3016) tensor(713.1740)\n",
      "tensor(855.6927) tensor(251.0450) tensor(1278.8766) tensor(712.7676)\n",
      "tensor(35.7684) tensor(546.1362) tensor(68.0528) tensor(615.5352)\n",
      "tensor(727.0663) tensor(472.6718) tensor(831.5149) tensor(526.0409)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 167.0ms\n",
      "Speed: 3.0ms preprocess, 167.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(342.7114) tensor(337.3828) tensor(785.0975) tensor(714.4966)\n",
      "tensor(847.5408) tensor(253.9157) tensor(1278.8413) tensor(711.1749)\n",
      "tensor(32.6670) tensor(546.1133) tensor(66.9701) tensor(620.4125)\n",
      "tensor(413.3384) tensor(479.5665) tensor(531.7962) tensor(625.4384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 tv, 1 cell phone, 138.0ms\n",
      "Speed: 3.1ms preprocess, 138.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(326.5621) tensor(312.1965) tensor(759.2062) tensor(712.9680)\n",
      "tensor(853.3079) tensor(249.2598) tensor(1278.7573) tensor(711.2230)\n",
      "tensor(415.6279) tensor(481.3461) tensor(515.4303) tensor(630.6275)\n",
      "tensor(33.6005) tensor(546.9491) tensor(66.6474) tensor(619.9098)\n",
      "tensor(120.9604) tensor(264.2385) tensor(223.6767) tensor(484.4127)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 141.3ms\n",
      "Speed: 3.0ms preprocess, 141.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(324.0598) tensor(160.0886) tensor(814.0796) tensor(714.7054)\n",
      "tensor(853.5663) tensor(251.3225) tensor(1278.7179) tensor(712.0631)\n",
      "tensor(324.4414) tensor(366.8835) tensor(809.9780) tensor(713.6793)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 bed, 1 cell phone, 142.1ms\n",
      "Speed: 2.0ms preprocess, 142.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(328.9775) tensor(374.0161) tensor(849.7386) tensor(714.6618)\n",
      "tensor(529.7249) tensor(456.6163) tensor(580.1636) tensor(506.1863)\n",
      "tensor(853.3956) tensor(253.3403) tensor(1278.8486) tensor(711.4075)\n",
      "tensor(30.7301) tensor(547.0813) tensor(68.1291) tensor(620.4373)\n",
      "tensor(849.2341) tensor(280.1706) tensor(1277.5801) tensor(712.8485)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bed, 141.2ms\n",
      "Speed: 3.0ms preprocess, 141.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(341.5413) tensor(381.1531) tensor(845.5796) tensor(714.1561)\n",
      "tensor(855.7598) tensor(251.8026) tensor(1278.7339) tensor(712.7445)\n",
      "tensor(852.3515) tensor(293.0218) tensor(1277.5361) tensor(712.7698)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 131.4ms\n",
      "Speed: 4.0ms preprocess, 131.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(338.2218) tensor(391.0150) tensor(857.4137) tensor(714.5031)\n",
      "tensor(855.0059) tensor(251.0846) tensor(1278.6851) tensor(711.6874)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 139.6ms\n",
      "Speed: 2.0ms preprocess, 139.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(340.1997) tensor(392.1433) tensor(864.1477) tensor(713.2670)\n",
      "tensor(32.1664) tensor(546.4948) tensor(66.6370) tensor(620.5455)\n",
      "tensor(839.6213) tensor(253.1647) tensor(1279.0212) tensor(711.2280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 bottle, 1 cell phone, 134.8ms\n",
      "Speed: 3.0ms preprocess, 134.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(342.8177) tensor(385.3691) tensor(856.6421) tensor(713.8821)\n",
      "tensor(452.6429) tensor(522.2369) tensor(538.2025) tensor(625.7404)\n",
      "tensor(341.5070) tensor(383.9065) tensor(703.6169) tensor(715.5558)\n",
      "tensor(871.5767) tensor(249.8799) tensor(1278.9614) tensor(712.0819)\n",
      "tensor(32.8667) tensor(546.9872) tensor(66.3901) tensor(621.4601)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cup, 145.0ms\n",
      "Speed: 3.0ms preprocess, 145.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(343.9347) tensor(378.7064) tensor(855.7395) tensor(714.2130)\n",
      "tensor(849.2134) tensor(254.7207) tensor(1279.1362) tensor(712.6440)\n",
      "tensor(31.6535) tensor(547.2590) tensor(66.6525) tensor(619.1113)\n",
      "tensor(69.3237) tensor(561.3163) tensor(109.1614) tensor(615.8495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 144.1ms\n",
      "Speed: 2.0ms preprocess, 144.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(708.4065) tensor(244.1862) tensor(1278.9647) tensor(713.6803)\n",
      "tensor(32.4452) tensor(547.2555) tensor(67.1985) tensor(622.8380)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 148.6ms\n",
      "Speed: 3.0ms preprocess, 148.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(680.0560) tensor(249.7493) tensor(1278.3765) tensor(712.5959)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 tv, 141.6ms\n",
      "Speed: 1.9ms preprocess, 141.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(447.1588) tensor(196.7482) tensor(910.9503) tensor(441.1733)\n",
      "tensor(30.5293) tensor(547.4261) tensor(67.4448) tensor(619.8541)\n",
      "tensor(774.0625) tensor(250.5400) tensor(1277.7258) tensor(713.3270)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 tv, 148.9ms\n",
      "Speed: 2.0ms preprocess, 148.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(817.9286) tensor(252.6331) tensor(1278.2797) tensor(712.5886)\n",
      "tensor(452.0240) tensor(171.9211) tensor(954.8833) tensor(429.2202)\n",
      "tensor(214.7790) tensor(216.5076) tensor(942.0949) tensor(713.8052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 tv, 143.1ms\n",
      "Speed: 3.0ms preprocess, 143.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(443.1300) tensor(155.8731) tensor(946.8820) tensor(428.7430)\n",
      "tensor(817.6187) tensor(251.5605) tensor(1278.0708) tensor(712.2440)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 2 tvs, 1 cell phone, 151.1ms\n",
      "Speed: 2.0ms preprocess, 151.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(438.2178) tensor(148.4562) tensor(936.7240) tensor(445.2236)\n",
      "tensor(837.2211) tensor(250.0377) tensor(1278.1334) tensor(710.1936)\n",
      "tensor(873.2356) tensor(444.5538) tensor(1059.5945) tensor(557.8263)\n",
      "tensor(566.9655) tensor(245.1089) tensor(1279.2076) tensor(711.3899)\n",
      "tensor(31.5130) tensor(547.3331) tensor(67.5096) tensor(618.9227)\n",
      "tensor(126.8305) tensor(254.4269) tensor(220.1483) tensor(515.1478)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 tv, 352.2ms\n",
      "Speed: 53.0ms preprocess, 352.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 128.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(386.4496) tensor(153.0704) tensor(980.2933) tensor(475.7592)\n",
      "tensor(837.4390) tensor(250.3803) tensor(1278.8970) tensor(710.5228)\n",
      "tensor(30.1941) tensor(547.6937) tensor(66.8444) tensor(621.9691)\n",
      "tensor(830.9010) tensor(248.2082) tensor(1279.1442) tensor(709.7712)\n",
      "tensor(346.9445) tensor(147.6286) tensor(1006.2936) tensor(471.5404)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 128.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 139.1ms\n",
      "Speed: 3.0ms preprocess, 139.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(832.2351) tensor(250.0274) tensor(1278.8433) tensor(710.6342)\n",
      "tensor(255.7646) tensor(150.8905) tensor(996.1316) tensor(473.0637)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 tv, 342.5ms\n",
      "Speed: 61.8ms preprocess, 342.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(840.7961) tensor(248.7153) tensor(1279.0679) tensor(710.4587)\n",
      "tensor(381.3769) tensor(148.4615) tensor(1003.1003) tensor(471.0811)\n",
      "tensor(641.5796) tensor(248.7435) tensor(1278.1367) tensor(713.5015)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 2 tvs, 149.8ms\n",
      "Speed: 3.1ms preprocess, 149.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(846.3555) tensor(249.3503) tensor(1279.0105) tensor(710.7893)\n",
      "tensor(380.3464) tensor(149.0396) tensor(1005.8501) tensor(459.6386)\n",
      "tensor(0.3620) tensor(157.1661) tensor(233.8316) tensor(563.4526)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 139.5ms\n",
      "Speed: 3.0ms preprocess, 139.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(843.8904) tensor(249.3372) tensor(1278.5769) tensor(712.2415)\n",
      "tensor(559.7983) tensor(249.0310) tensor(1277.8461) tensor(712.1055)\n",
      "tensor(30.8596) tensor(545.4850) tensor(67.1632) tensor(619.7708)\n",
      "tensor(982.8075) tensor(442.5977) tensor(1072.4606) tensor(524.2254)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 tv, 141.3ms\n",
      "Speed: 2.0ms preprocess, 141.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(832.5145) tensor(245.3226) tensor(1278.8895) tensor(710.8766)\n",
      "tensor(367.9128) tensor(146.7948) tensor(1003.6425) tensor(455.9022)\n",
      "tensor(33.2501) tensor(547.6532) tensor(65.7438) tensor(617.3998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 tv, 1 cell phone, 150.2ms\n",
      "Speed: 2.0ms preprocess, 150.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(843.2125) tensor(250.3661) tensor(1278.5682) tensor(712.5583)\n",
      "tensor(967.0668) tensor(451.8195) tensor(1075.1029) tensor(532.2072)\n",
      "tensor(345.2809) tensor(154.1432) tensor(982.3342) tensor(478.9752)\n",
      "tensor(31.1778) tensor(547.1053) tensor(67.4793) tensor(621.0046)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 bottle, 1 tv, 1 cell phone, 133.6ms\n",
      "Speed: 3.0ms preprocess, 133.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(817.5211) tensor(250.0859) tensor(1278.2222) tensor(711.1861)\n",
      "tensor(71.1115) tensor(60.6212) tensor(1110.7706) tensor(712.9946)\n",
      "tensor(980.5678) tensor(456.1278) tensor(1075.0098) tensor(536.6603)\n",
      "tensor(260.7544) tensor(103.1714) tensor(1251.6008) tensor(716.2114)\n",
      "tensor(30.1048) tensor(547.3696) tensor(66.4123) tensor(620.3651)\n",
      "tensor(217.3645) tensor(150.4646) tensor(808.9402) tensor(592.8829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 tv, 1 cell phone, 132.3ms\n",
      "Speed: 2.0ms preprocess, 132.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(408.4612) tensor(295.3241) tensor(1091.1213) tensor(714.0763)\n",
      "tensor(31.1288) tensor(547.9806) tensor(66.8473) tensor(622.4910)\n",
      "tensor(1013.6824) tensor(622.5450) tensor(1092.7705) tensor(716.2482)\n",
      "tensor(0.6637) tensor(301.2300) tensor(153.7092) tensor(591.5191)\n",
      "tensor(853.2709) tensor(249.9202) tensor(1278.4376) tensor(713.5480)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 bottle, 1 chair, 2 cell phones, 136.0ms\n",
      "Speed: 2.0ms preprocess, 136.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(616.7791) tensor(396.4369) tensor(699.4373) tensor(442.4505)\n",
      "tensor(425.4267) tensor(292.0105) tensor(1075.1697) tensor(713.9298)\n",
      "tensor(364.0661) tensor(620.1526) tensor(478.3550) tensor(720.)\n",
      "tensor(829.4291) tensor(248.0593) tensor(1278.6630) tensor(712.4300)\n",
      "tensor(425.3109) tensor(290.4402) tensor(846.4483) tensor(713.2920)\n",
      "tensor(30.9450) tensor(547.5483) tensor(65.4790) tensor(621.9375)\n",
      "tensor(953.7627) tensor(475.7981) tensor(1060.8073) tensor(536.3127)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 bottle, 1 tv, 1 cell phone, 131.1ms\n",
      "Speed: 2.0ms preprocess, 131.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(813.4730) tensor(249.2546) tensor(1278.6136) tensor(713.3107)\n",
      "tensor(360.6809) tensor(292.1570) tensor(1074.9978) tensor(714.5251)\n",
      "tensor(276.4031) tensor(290.5945) tensor(838.9071) tensor(714.0629)\n",
      "tensor(30.8205) tensor(546.5433) tensor(67.0327) tensor(620.5360)\n",
      "tensor(0.2919) tensor(280.5607) tensor(219.1526) tensor(673.2168)\n",
      "tensor(919.8157) tensor(477.8164) tensor(1059.1575) tensor(551.4681)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 chair, 2 cell phones, 142.1ms\n",
      "Speed: 3.0ms preprocess, 142.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(805.5054) tensor(251.1706) tensor(1278.7944) tensor(713.6389)\n",
      "tensor(488.1036) tensor(288.6317) tensor(1067.7781) tensor(713.5297)\n",
      "tensor(915.8933) tensor(479.6381) tensor(1057.2988) tensor(547.5343)\n",
      "tensor(534.2487) tensor(403.7395) tensor(664.0924) tensor(607.6859)\n",
      "tensor(364.5589) tensor(618.9716) tensor(479.8923) tensor(720.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 2 cell phones, 126.1ms\n",
      "Speed: 3.0ms preprocess, 126.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(806.4619) tensor(251.1561) tensor(1278.6829) tensor(713.0303)\n",
      "tensor(405.0221) tensor(282.0495) tensor(1060.8965) tensor(713.5377)\n",
      "tensor(914.5875) tensor(479.6427) tensor(1051.4354) tensor(529.5858)\n",
      "tensor(535.3793) tensor(412.6093) tensor(618.3434) tensor(454.7567)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 126.0ms\n",
      "Speed: 2.0ms preprocess, 126.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(425.2246) tensor(283.2649) tensor(1064.2184) tensor(717.3824)\n",
      "tensor(812.1881) tensor(249.4395) tensor(1278.9530) tensor(711.4012)\n",
      "tensor(30.5332) tensor(548.1525) tensor(66.7734) tensor(621.0975)\n",
      "tensor(522.6172) tensor(401.6785) tensor(601.2659) tensor(470.5786)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 166.8ms\n",
      "Speed: 2.0ms preprocess, 166.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(399.8015) tensor(284.1466) tensor(1037.7410) tensor(714.1522)\n",
      "tensor(807.0823) tensor(247.8987) tensor(1278.8948) tensor(711.5037)\n",
      "tensor(31.9875) tensor(547.4528) tensor(65.9542) tensor(618.8563)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 1 person, 1 bottle, 1 cell phone, 133.4ms\n",
      "Speed: 2.0ms preprocess, 133.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 131.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(577.1432) tensor(249.0695) tensor(1278.7953) tensor(713.4097)\n",
      "tensor(529.8378) tensor(383.1728) tensor(668.6815) tensor(586.0829)\n",
      "tensor(30.9241) tensor(546.3947) tensor(67.9258) tensor(619.5498)\n",
      "tensor(322.0379) tensor(274.5997) tensor(974.5163) tensor(713.5707)\n",
      "tensor(31.4264) tensor(548.0269) tensor(66.3391) tensor(617.6311)\n",
      "tensor(894.9640) tensor(252.5309) tensor(1278.8480) tensor(710.6282)\n",
      "tensor(644.8113) tensor(243.7720) tensor(1279.8563) tensor(712.0173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 131.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 126.9ms\n",
      "Speed: 3.2ms preprocess, 126.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 129.7ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(318.8837) tensor(266.0742) tensor(956.4154) tensor(713.8220)\n",
      "tensor(861.5381) tensor(253.1108) tensor(1278.5664) tensor(711.8437)\n",
      "tensor(31.5773) tensor(547.8195) tensor(66.5079) tensor(619.8196)\n",
      "tensor(317.8958) tensor(253.3975) tensor(929.0447) tensor(712.7201)\n",
      "tensor(875.2456) tensor(248.5549) tensor(1278.8987) tensor(711.1079)\n",
      "tensor(32.5934) tensor(542.0374) tensor(66.8548) tensor(613.4233)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 129.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cell phone, 126.7ms\n",
      "Speed: 3.0ms preprocess, 126.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(310.6012) tensor(236.3590) tensor(932.5432) tensor(713.5782)\n",
      "tensor(565.7081) tensor(363.5624) tensor(649.5497) tensor(419.0534)\n",
      "tensor(890.4882) tensor(248.0599) tensor(1278.7357) tensor(711.8304)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 131.8ms\n",
      "Speed: 3.0ms preprocess, 131.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(297.6830) tensor(233.0002) tensor(928.4547) tensor(711.4579)\n",
      "tensor(893.0990) tensor(247.9638) tensor(1279.0985) tensor(709.3984)\n",
      "tensor(31.8921) tensor(538.4628) tensor(70.7045) tensor(611.2897)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 bottle, 129.9ms\n",
      "Speed: 2.0ms preprocess, 129.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 132.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(295.3215) tensor(232.5568) tensor(920.9575) tensor(714.4500)\n",
      "tensor(888.9960) tensor(249.4333) tensor(1278.6100) tensor(712.4886)\n",
      "tensor(32.4921) tensor(539.1677) tensor(67.7542) tensor(611.6208)\n",
      "tensor(297.5919) tensor(231.1737) tensor(918.8964) tensor(714.0577)\n",
      "tensor(893.3693) tensor(246.0291) tensor(1278.5798) tensor(711.9202)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 132.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 130.2ms\n",
      "Speed: 3.0ms preprocess, 130.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(295.3615) tensor(234.0249) tensor(933.7593) tensor(710.5773)\n",
      "tensor(880.4454) tensor(248.1068) tensor(1278.6569) tensor(713.1759)\n",
      "tensor(32.8981) tensor(536.7307) tensor(67.4900) tensor(607.5317)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 bottle, 1 tv, 131.2ms\n",
      "Speed: 2.0ms preprocess, 131.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(292.8866) tensor(235.1939) tensor(928.9879) tensor(712.2780)\n",
      "tensor(877.0551) tensor(247.0598) tensor(1278.9194) tensor(712.2556)\n",
      "tensor(0.2017) tensor(287.8901) tensor(218.5899) tensor(536.2144)\n",
      "tensor(999.1530) tensor(248.4655) tensor(1279.1730) tensor(712.0504)\n",
      "tensor(31.1576) tensor(542.2758) tensor(65.6877) tensor(614.0673)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cell phone, 123.1ms\n",
      "Speed: 4.0ms preprocess, 123.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 123.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(292.8961) tensor(238.6273) tensor(925.7598) tensor(713.4213)\n",
      "tensor(563.2620) tensor(380.1129) tensor(682.4836) tensor(457.6894)\n",
      "tensor(874.6589) tensor(239.8162) tensor(1278.6296) tensor(711.6412)\n",
      "tensor(1002.0725) tensor(239.0941) tensor(1278.9478) tensor(711.3557)\n",
      "tensor(297.4671) tensor(243.8025) tensor(927.1147) tensor(714.8367)\n",
      "tensor(877.8217) tensor(233.3340) tensor(1278.5723) tensor(712.3608)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Speed: 3.0ms preprocess, 123.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 158.0ms\n",
      "Speed: 2.0ms preprocess, 158.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(298.4300) tensor(245.5046) tensor(937.6406) tensor(714.6024)\n",
      "tensor(887.1050) tensor(256.9371) tensor(1278.5876) tensor(711.4741)\n",
      "tensor(33.7827) tensor(542.1282) tensor(67.7209) tensor(612.6819)\n",
      "tensor(574.1531) tensor(382.2826) tensor(721.2657) tensor(475.1042)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 139.2ms\n",
      "Speed: 2.0ms preprocess, 139.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(296.8583) tensor(240.5421) tensor(932.2560) tensor(713.3612)\n",
      "tensor(881.7902) tensor(249.4905) tensor(1278.8752) tensor(711.6245)\n",
      "tensor(581.4422) tensor(382.5589) tensor(693.2448) tensor(452.0200)\n",
      "tensor(305.0501) tensor(240.0405) tensor(933.2615) tensor(712.6722)\n",
      "tensor(893.2649) tensor(249.6364) tensor(1278.9978) tensor(710.3579)\n",
      "tensor(577.8436) tensor(377.1787) tensor(733.7719) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 1 cell phone, 128.2ms\n",
      "Speed: 2.0ms preprocess, 128.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(474.6839)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 bottle, 149.1ms\n",
      "Speed: 4.0ms preprocess, 149.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(303.4235) tensor(242.3171) tensor(935.8209) tensor(712.6312)\n",
      "tensor(887.2769) tensor(245.3128) tensor(1278.6780) tensor(711.4986)\n",
      "tensor(32.1371) tensor(536.7063) tensor(68.0407) tensor(608.7688)\n",
      "tensor(1008.4207) tensor(245.3803) tensor(1278.9744) tensor(710.6864)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 140.7ms\n",
      "Speed: 2.0ms preprocess, 140.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(312.9258) tensor(245.0304) tensor(934.9937) tensor(710.2584)\n",
      "tensor(877.3125) tensor(246.0886) tensor(1278.4531) tensor(712.3979)\n",
      "tensor(299.4575) tensor(242.4052) tensor(936.6658) tensor(714.1689)\n",
      "tensor(477.1132) tensor(239.8384) tensor(932.1531) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 persons, 1 cell phone, 120.8ms\n",
      "Speed: 3.0ms preprocess, 120.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(704.0439)\n",
      "tensor(884.7300) tensor(243.8884) tensor(1278.4927) tensor(712.5055)\n",
      "tensor(538.3290) tensor(378.8520) tensor(739.6097) tensor(648.7994)\n",
      "tensor(301.9945) tensor(236.0670) tensor(933.7577) tensor(712.2353)\n",
      "tensor(882.5444) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: 384x640 2 persons, 133.2ms\n",
      "Speed: 3.0ms preprocess, 133.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(235.0958) tensor(1278.6042) tensor(712.3254)\n"
     ]
    }
   ],
   "source": [
    "#####TESTINGGGG#############\n",
    "#importing cvzone to show detections\n",
    "import cvzone\n",
    "import numpy as np\n",
    "#webcam object\n",
    "cam=cv2.VideoCapture(0)\n",
    "#width\n",
    "cam.set(3, 1280)\n",
    "#height\n",
    "cam.set(4, 720)\n",
    "\n",
    "#LOWER AND UPPER RANGE OF MULTIPLE COLORS\n",
    "\n",
    "lower_black = np.array([0,0,0])\n",
    "upper_black = np.array([250,255,30])\n",
    "\n",
    "lower_white = np.array([0,0,255])\n",
    "upper_white = np.array([0,0,255])\n",
    "\n",
    "lower_red = np.array([0,150,50])\n",
    "upper_red = np.array([10,255,255])\n",
    "\n",
    "lower_green = np.array([45,150,50])\n",
    "upper_green = np.array([65,255,255])\n",
    "\n",
    "lower_yellow = np.array([25,150,50])\n",
    "upper_yellow = np.array([35,255,255])\n",
    "\n",
    "lower_light_blue = np.array([95,150,0])\n",
    "upper_light_blue = np.array([110,255,255])\n",
    "\n",
    "lower_orange = np.array([15,150,0])\n",
    "upper_orange = np.array([25,255,255])\n",
    "\n",
    "lower_dark_pink = np.array([160,150,0])\n",
    "upper_dark_pink = np.array([170,255,255])\n",
    "\n",
    "lower_pink = np.array([145,150,0])\n",
    "upper_pink = np.array([155,255,255])\n",
    "\n",
    "lower_cyan = np.array([85,150,0])\n",
    "upper_cyan = np.array([95,255,255])\n",
    "\n",
    "lower_dark_blue = np.array([115,150,0])\n",
    "upper_dark_blue = np.array([125,255,255])\n",
    "\n",
    "#purple\n",
    "lower_purple = np.array([129, 50, 70])\n",
    "#np.array([129, 50, 70])138,43,226\n",
    "upper_purple = np.array([158, 255, 255])\n",
    "#np.array([158, 255, 255])255,0,255\n",
    "#light purple\n",
    "#upper_light_purple = np.array([220,208,255])\n",
    "#lower_light_purple = np.array([181,126,220])\n",
    "#dark purple\n",
    "#upper_dark_purple = np.array([145,92,131])\n",
    "#lower_dark_purple = np.array([50,23,77])\n",
    "\n",
    "\n",
    "#turqise\n",
    "lower_turq = np.array([0,206,209])\n",
    "upper_turq = np.array([175,238,238])\n",
    "\n",
    "\n",
    "\n",
    "#TO KEEP THE WEBCAM RUNNING\n",
    "while True:\n",
    "    #IF CAM IS FOUND, SAVE IT TO \"img\"\n",
    "    success, img = cam.read()\n",
    "    #YOLO MODEL RESULTS\n",
    "    results=model(img, stream=True)\n",
    "    #CAPTURE COLORS AND TURN THEM TO \"HSV\" VALUES\n",
    "    imgC = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    #range of colors\n",
    "    yellow = cv2.inRange(imgC, lower_yellow, upper_yellow)\n",
    "    red = cv2.inRange(imgC, lower_red, upper_red)\n",
    "    dblue = cv2.inRange(imgC, lower_dark_blue, upper_dark_blue)\n",
    "    green = cv2.inRange(imgC, lower_green, upper_green)\n",
    "    pink = cv2.inRange(imgC, lower_pink, upper_pink)\n",
    "    lblue = cv2.inRange(imgC, lower_light_blue, upper_light_blue)\n",
    "    purple = cv2.inRange(imgC, lower_purple, upper_purple)\n",
    "    orange = cv2.inRange(imgC, lower_orange, upper_orange)\n",
    "    cyan = cv2.inRange(imgC, lower_cyan, upper_cyan)\n",
    "    turq = cv2.inRange(imgC, lower_turq, upper_turq)\n",
    "    \n",
    "    #lpurple = cv2.inRange(imgC, lower_light_purple, upper_light_purple)\n",
    "    #dpurple = cv2.inRange(imgC, lower_dark_purple, upper_dark_purple)\n",
    "    #morphological transformation and dilation\n",
    "    #kernal=np.ones((5,5), 'uint8')\n",
    "    #red=cv2.dilate(red, kernal)\n",
    "    #resr=cv2.bitwise_and(img, img, mask = red)\n",
    "    #blue=cv2.dilate(blue, kernal)\n",
    "    #resb=cv2.bitwise_and(img, img, mask = blue)\n",
    "    #yellow=cv2.dilate(yellow, kernal)\n",
    "    #resy=cv2.bitwise_and(img, img, mask = red)\n",
    "    \n",
    "    #tracking colors\n",
    "    (contours, hierarchy)=cv2.findContours(red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'red', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "    \n",
    "    #BLUE\n",
    "    (contours, hierarchy)=cv2.findContours(dblue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'dark blue', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "    #YELLOW\n",
    "    (contours, hierarchy)=cv2.findContours(yellow, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'yellow', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "            \n",
    "    #green\n",
    "    (contours, hierarchy)=cv2.findContours(green, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'green', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "    #pink\n",
    "    (contours, hierarchy)=cv2.findContours(pink, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'pink', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "    #lblue\n",
    "    (contours, hierarchy)=cv2.findContours(lblue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'light blue', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "    \n",
    "    #purple\n",
    "    (contours, hierarchy)=cv2.findContours(purple, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'purple', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "    #ORANGE\n",
    "    (contours, hierarchy)=cv2.findContours(orange, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'orange', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "    #CYAN\n",
    "    (contours, hierarchy)=cv2.findContours(cyan, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'cyan', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "            \n",
    "    #TURQ\n",
    "    (contours, hierarchy)=cv2.findContours(turq, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'turq', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "    #cv2.imshow('mask', mask)\n",
    "    #cv2.imshow('mask', mask)\n",
    "    \n",
    "    #cv2.imshow('mask', mask)\n",
    "    #contour, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #if len(contour)!=0:\n",
    "    #    for cont in contour:\n",
    "    #        if cv2.contourArea(cont)>1000:\n",
    "    #            x, y, w, h=cv2.boundingRect(cont)\n",
    "    #            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 3)\n",
    "    #            cv2.putText(img, 'yellow', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (251,255,255), 2)\n",
    "    #cam.set(CV_CAP_PROP_BUFFERSIZE, 3)\n",
    "    #results.boxes\n",
    "    #IF CAN NOT THERE\n",
    "    if not cam.isOpened():\n",
    "        print('Unable to load camera.')\n",
    "        sleep(5)\n",
    "        pass\n",
    "    #FOR CAPTURING BOUNDRIES OF THE YOLO PREDICTIONS \n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cam.read()\n",
    "    #end of extra\n",
    "    #loop through every frame\n",
    "    for r in results:\n",
    "        boxes=r.boxes\n",
    "        #get Xs and Ys of each box\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            print(x1, y1, x2, y2)\n",
    "    #cv2.imshow(\"image\", img)\n",
    "    #TO QUIT WEBCAM, PRESS \"q\" (SOMETIMES YOU HAVE TO HOLD IT AND OTHER TIMES YOU CAN JUST PRESS IT)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45d39bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34da866",
   "metadata": {},
   "source": [
    "#####TESTINGGGG#############\n",
    "#importing cvzone to show detections\n",
    "import cvzone\n",
    "import numpy as np\n",
    "#webcam object\n",
    "cam=cv2.VideoCapture(0)\n",
    "#width\n",
    "cam.set(3, 1280)\n",
    "#height\n",
    "cam.set(4, 720)\n",
    "\n",
    "####colors#####\n",
    "\n",
    "#YELLOW\n",
    "#lower range of yellow\n",
    "#yellow_lower = np.array([15, 150, 20])\n",
    "#upper range of yellow\n",
    "#yellow_upper = np.array([35, 255, 255])\n",
    "yellow_lower = np.array([15, 150, 20])\n",
    "#upper range of yellow\n",
    "yellow_upper = np.array([35, 255, 255])\n",
    "#BLUE\n",
    "#lower range of blue\n",
    "#blue_lower = np.array([99, 115, 150])\n",
    "#upper range of blue\n",
    "#blue_upper = np.array([60, 255, 255])\n",
    "blue_lower = np.array([0, 25, 125])\n",
    "#upper range of blue\n",
    "blue_upper = np.array([183, 220, 255])\n",
    "#RED\n",
    "#lower range of red\n",
    "#red_lower = np.array([136, 87, 111])\n",
    "#upper range of red\n",
    "#red_upper = np.array([180, 255, 255])\n",
    "red_lower = np.array([124, 0, 0])\n",
    "#upper range of red\n",
    "red_upper = np.array([255, 67, 67])\n",
    "#PINK\n",
    "pink_lower = np.array([164, 43, 43])\n",
    "#upper range of red\n",
    "pink_upper = np.array([255, 199, 199])\n",
    "#PURPLE\n",
    "#GREEN\n",
    "green_lower = np.array([0, 48, 20])\n",
    "#upper range of yellow\n",
    "green_upper = np.array([210, 255, 226])\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cam.read()\n",
    "    results=model(img, stream=True)\n",
    "    imgC = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    #range of colors\n",
    "    yellow = cv2.inRange(imgC, yellow_lower, yellow_upper)\n",
    "    red = cv2.inRange(imgC, red_lower, red_upper)\n",
    "    blue = cv2.inRange(imgC, blue_lower, blue_upper)\n",
    "    #morphological transformation and dilation\n",
    "    #kernal=np.ones((5,5), 'uint8')\n",
    "    #red=cv2.dilate(red, kernal)\n",
    "    #resr=cv2.bitwise_and(img, img, mask = red)\n",
    "    #blue=cv2.dilate(blue, kernal)\n",
    "    #resb=cv2.bitwise_and(img, img, mask = blue)\n",
    "    #yellow=cv2.dilate(yellow, kernal)\n",
    "    #resy=cv2.bitwise_and(img, img, mask = red)\n",
    "    \n",
    "    #tracking colors\n",
    "    (contours, hierarchy)=cv2.findContours(red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'red', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "    #BLUE\n",
    "    (contours, hierarchy)=cv2.findContours(blue, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'blue', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "    #YELLOW\n",
    "    (contours, hierarchy)=cv2.findContours(yellow, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for pic, conn in enumerate(contours):\n",
    "        area=cv2.contourArea(conn)\n",
    "        if(area>500):\n",
    "            x, y, w, h = cv2.boundingRect(conn)\n",
    "            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 2)\n",
    "            cv2.putText(img, 'yellow', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (251,255,255), 2)\n",
    "            \n",
    "    \n",
    "    #cv2.imshow('mask', mask)\n",
    "    #contour, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #if len(contour)!=0:\n",
    "    #    for cont in contour:\n",
    "    #        if cv2.contourArea(cont)>1000:\n",
    "    #            x, y, w, h=cv2.boundingRect(cont)\n",
    "    #            cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 3)\n",
    "    #            cv2.putText(img, 'yellow', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (251,255,255), 2)\n",
    "    #cam.set(CV_CAP_PROP_BUFFERSIZE, 3)\n",
    "    #results.boxes\n",
    "    #extra\n",
    "    if not cam.isOpened():\n",
    "        print('Unable to load camera.')\n",
    "        sleep(5)\n",
    "        pass\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cam.read()\n",
    "    #end of extra\n",
    "    #loop through every frame\n",
    "    for r in results:\n",
    "        boxes=r.boxes\n",
    "        #get Xs and Ys of each box\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            print(x1, y1, x2, y2)\n",
    "    #cv2.imshow(\"image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "    #if cv2.getWindowProperty(img, cv2.WND_PROP_VISIBLE) <1:\n",
    "        #break\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4b390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c6a7cfe",
   "metadata": {},
   "source": [
    "#importing cvzone to show detections\n",
    "import cvzone\n",
    "import numpy as np\n",
    "#webcam object\n",
    "cam=cv2.VideoCapture(0)\n",
    "#width\n",
    "cam.set(3, 1280)\n",
    "#height\n",
    "cam.set(4, 720)\n",
    "#lower range of color\n",
    "lower = np.array([15, 150, 20])\n",
    "#upper range of color\n",
    "upper = np.array([35, 255, 255])\n",
    "while True:\n",
    "    success, img = cam.read()\n",
    "    results=model(img, stream=True)\n",
    "    imgC = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    mask = cv2.inRange(imgC, lower, upper)\n",
    "    cv2.imshow('mask', mask)\n",
    "    contour, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(contour)!=0:\n",
    "        for cont in contour:\n",
    "            if cv2.contourArea(cont)>1000:\n",
    "                x, y, w, h=cv2.boundingRect(cont)\n",
    "                cv2.rectangle(img, (x, y), (x+w,y+h), (251, 255, 255), 3)\n",
    "                cv2.putText(img, 'yellow', (x, y+h+10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (251,255,255), 2)\n",
    "    #cam.set(CV_CAP_PROP_BUFFERSIZE, 3)\n",
    "    #results.boxes\n",
    "    #extra\n",
    "    if not cam.isOpened():\n",
    "        print('Unable to load camera.')\n",
    "        sleep(5)\n",
    "        pass\n",
    "\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cam.read()\n",
    "    #end of extra\n",
    "    #loop through every frame\n",
    "    for r in results:\n",
    "        boxes=r.boxes\n",
    "        #get Xs and Ys of each box\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            print(x1, y1, x2, y2)\n",
    "    #cv2.imshow(\"image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "    #if cv2.getWindowProperty(img, cv2.WND_PROP_VISIBLE) <1:\n",
    "        #break\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ce70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38165982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\anacnda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in d:\\anacnda3\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: torchaudio in d:\\anacnda3\\lib\\site-packages (2.0.2+cu118)\n",
      "Requirement already satisfied: sympy in d:\\anacnda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in d:\\anacnda3\\lib\\site-packages (from torch) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\anacnda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: filelock in d:\\anacnda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anacnda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: requests in d:\\anacnda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anacnda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: numpy in d:\\anacnda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anacnda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anacnda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anacnda3\\lib\\site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anacnda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anacnda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anacnda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d99c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d05798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33da6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch._C.has_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c83ff666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [39 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\_vendor\\packaging\\requirements.py\", line 35, in __init__\n",
      "      parsed = parse_requirement(requirement_string)\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 64, in parse_requirement\n",
      "      return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 82, in _parse_requirement\n",
      "      url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 126, in _parse_requirement_details\n",
      "      marker = _parse_requirement_marker(\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\_vendor\\packaging\\_parser.py\", line 147, in _parse_requirement_marker\n",
      "      tokenizer.raise_syntax_error(\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\_vendor\\packaging\\_tokenizer.py\", line 163, in raise_syntax_error\n",
      "      raise ParserSyntaxError(\n",
      "  setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "      python_version>\"3.7\"\n",
      "                    ^\n",
      "  \n",
      "  The above exception was the direct cause of the following exception:\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\fai-w\\AppData\\Local\\Temp\\pip-install-fzs69dxh\\tensorflow-gpu_ac75c62eb041483d9b2dd4d26558e24d\\setup.py\", line 40, in <module>\n",
      "      setuptools.setup()\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\__init__.py\", line 106, in setup\n",
      "      _install_setup_requires(attrs)\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\__init__.py\", line 77, in _install_setup_requires\n",
      "      dist.parse_config_files(ignore_option_errors=True)\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\dist.py\", line 910, in parse_config_files\n",
      "      self._finalize_requires()\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\dist.py\", line 607, in _finalize_requires\n",
      "      self._move_install_requirements_markers()\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\dist.py\", line 647, in _move_install_requirements_markers\n",
      "      inst_reqs = list(_reqs.parse(spec_inst_reqs))\n",
      "    File \"D:\\anacnda3\\envs\\cudaenv\\lib\\site-packages\\setuptools\\_vendor\\packaging\\requirements.py\", line 37, in __init__\n",
      "      raise InvalidRequirement(str(e)) from e\n",
      "  setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "      python_version>\"3.7\"\n",
      "                    ^\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "#pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dffa1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 16 02:06:49 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.14                 Driver Version: 531.14       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050 Ti    WDDM | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   46C    P8               N/A /  N/A|     68MiB /  4096MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2832    C+G   D:\\anacnda3\\envs\\cudaenv\\python.exe       N/A      |\n",
      "|    0   N/A  N/A      4088    C+G   ...ience\\NVIDIA GeForce Experience.exe    N/A      |\n",
      "|    0   N/A  N/A      8208    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     20516    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     21432    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     25996    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3496e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
